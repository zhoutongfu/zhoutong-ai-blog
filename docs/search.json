[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhoutong | AI Research Log",
    "section": "",
    "text": "Documenting hands-on explorations in LLM architecture and post-training. I focus on bridging deep research insights with the practical constraints of large-scale production systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Rise of Static Memory in LLMs\n\n\n\nLLM\n\nArchitecture\n\nMemory\n\n\n\n\n\n\n\n\n\nFeb 1, 2026\n\n\nZhoutong\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/static_llm_memory/index.html",
    "href": "posts/static_llm_memory/index.html",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Topics: LLM Knowledge, Embedding Scaling, Sparse Architectures\n\n\n\n\nImproving reasoning remains a primary goal for modern AI, yet recent findings (Zhang et al., 2025 [8]) highlight a critical bottleneck: a model’s reasoning potential is capped by the knowledge “primitives” absorbed during Pre-Training. While Post-Training can refine how a model thinks, it cannot fabricate knowledge from scratch; it merely optimizes the foundation laid earlier. Consequently, achieving substantial breakthroughs in reasoning requires massively scaling the pre-training stage to embed a far richer set of facts and primitives.\n\n\n\nStandard Transformers face a major inefficiency when scaling knowledge. Interpretability research from Anthropic [7, 1] reveals that dense models do not store information in a static database; instead, they dynamically “compute” it. Retrieving a simple concept like “The Golden Gate Bridge” triggers a complex, expensive sequence of feature interactions across layers. This architecture conflates storage with processing, forcing the model to burn valuable computational resources (FLOPs) just to “remember” rather than to “think”.\n\n\n\n\nModern LLMs traditionally conflate “reasoning” (dynamic computation) with “knowledge” (static fact retrieval). We use the same expensive matrix multiplications to calculate math problems as we do to recall that “Paris is the capital of France”.\nFour emerging architectures—STEM (Meta/CMU) [6], PLE (Google DeepMind) [3], Engram (DeepSeek) [2], and LongCat (Meituan) [5]—address this by moving static information into Embedding Modules. Instead of computing facts, these models retrieve them. This shift decouples model capacity from inference cost, allowing for massive “memory” scaling without exploding GPU requirements.\n\n\nComparison of Static Memory Architectures\n\n\n\n\n\n\n\n\n\nFeature\nGemma 3n PLE (DeepMind)\nSTEM (Meta/CMU)\nEngram (DeepSeek)\nLongCat (Meituan)\n\n\n\n\nDate\nJune 26, 2025\nJan 15, 2026\nJan 12, 2026\nJan 29, 2026\n\n\nCore Concept\nAugmentation: Adds extra embeddings to modulate/augment layers.\nReplacement: Swaps FFN Up-Projection for a Lookup Table.\nComplement: Adds a “Memory Module” alongside the Neural Backbone.\nInput Augmentation: Expands the model’s vocabulary/input capacity rather than depth.\n\n\nMechanism\nPer-Layer Embeddings (Streamed)\nToken-indexed Lookup (Static)\nHashed N-Gram Lookup (Contextual)\nInput-Level N-Gram Lookup: Averages N-gram vectors into the input.\n\n\nHardware Goal\nOn-Device Efficiency (Low VRAM)\nTraining Stability & Interpretability\nMassive Scaling (Memory vs. Logic)\nCache Efficiency: Uses “N-gram Caching” to bypass communication bottlenecks.\n\n\nKey Win\nRunning 8B models on 3GB RAM phones\n“Surgical” Knowledge Editing\n+5.0 on Reasoning (BBH) by offloading facts\nPareto Efficiency: Beats parameter-equivalent MoEs on Coding & Agentic tasks.\n\n\nModel Size\n2B (Effective 4B)\n1B (Evaluated at 350M & 1B)\n40B (Tested up to 100B Engram module)\n68.5B (~31B of which are static N-gram embeddings)\n\n\n\n\n\n\n\n\n\nGemma 3n is engineered specifically for mobile/edge deployment [3]. Its goal is to fit a model with “8B parameter intelligence” onto a device that only has RAM for a 2B model.\n\nPLE (Per Layer Embeddings): Instead of storing all weights in the GPU/NPU memory (VRAM), it keeps massive embedding tables in the slower System RAM (CPU).\nStreaming: As the neural network processes layer \\(i\\), the specific embedding for that layer is streamed from the CPU to the NPU just in time.\nMechanism: PLEs do not usually replace the FFN. Instead, they modulate the residual stream or the FFN output. The STEM paper notes that PLEs are often much lower dimension (e.g., 256 dim) compared to the model’s width.\n\n\n\n\nThe Per-Layer Embedding (PLE) mechanism works by retrieving a massive, token-specific embedding vector that is pre-sliced to provide a unique “knowledge” input for every decoder layer [9]. As the model processes a token, each layer accesses its specific slice and dynamically gates it using the current hidden state—essentially using the active context to determine how much of the static retrieved knowledge to admit.\n# The massive \"Memory Bank\" storing all layer-specific vectors for every token.\n# Total Size: Vocab_PLE x (Num_Layers * PLE_Dim)\n# Example Dimensions: 2M tokens x (40 layers * 256 dim)\nself.embed_tokens_per_layer = Gemma3nTextScaledWordEmbedding(\n    config.vocab_size_per_layer_input,\n    config.num_hidden_layers * config.hidden_size_per_layer_input,\n    self.padding_idx,\n    embed_scale=config.hidden_size_per_layer_input**0.5,\n)\n\ndef get_per_layer_inputs(self, input_ids: torch.LongTensor) -&gt; torch.Tensor:\n    # Look up the massive vector and \"slice\" it for each layer.\n    # Input:  [Batch, Seq_Len]\n    # Output: [Batch, Seq_Len, Num_Layers, PLE_Dim]\n    return self.embed_tokens_per_layer(input_ids).reshape(\n        *input_ids.shape,\n        self.config.num_hidden_layers,\n        self.hidden_size_per_layer_input,\n    )\n\n# Inside the decoder loop, we pass the specific slice for this layer:\n# per_layer_input = per_layer_inputs[:, :, layer_idx, :] \n# Shape: [Batch, Seq_Len, PLE_Dim]\n\n# ... inside the layer's forward pass ...\n\n# A. GATE: Use the current context (hidden state) to determine \"how much\" memory to read.\n# Project from Model_Dim (e.g., 2560) -&gt; PLE_Dim (e.g., 256)\nfirst_prediction = self.per_layer_input_gate(first_prediction)\nfirst_prediction = self.act_fn(first_prediction)\n\n# B. MODULATE: Inject the retrieved static memory via element-wise multiplication.\n# Dynamic Context (Gate) * Static Memory (PLE Vector)\nfirst_prediction = torch.multiply(first_prediction, per_layer_input)\n\n# C. PROJECT: Mix the result back into the main residual stream.\n# Project from PLE_Dim (e.g., 256) -&gt; Model_Dim (e.g., 2560)\nfirst_prediction = self.per_layer_projection(first_prediction)\nfirst_prediction = self.post_per_layer_input_norm(first_prediction)\n\n# D. ADD: Add to the model's prediction stream\ncorrected_predictions[1:] += first_prediction\n\n\n\n\nVRAM Breakthrough: Allows running high-capacity models on phones with 3-4GB of RAM.\nParameters vs. Compute: It massively increases the parameter count (knowledge capacity) without increasing the FLOPs (compute cost) or the active VRAM usage.\n\n\n\n\n\n\n\nSTEM [6] identifies that the Up-Projection matrix in a Feed-Forward Network (FFN) acts largely as a “Key” lookup in a Key-Value memory system.\n\nChange: It replaces the dense Up-Projection matrix (\\(W^u\\)) with a static, token-indexed embedding table (\\(U_{token_id}\\)).\nRetention: It keeps the Gate Projection (\\(W^g\\)) and Down Projection (\\(W^d\\)) as dense layers.\nFormula: \\(y = W_{down}(\\text{SiLU}(W_{gate}x) \\odot \\text{Lookup}(TokenID))\\)\n\n Figure: Schematics of (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM.\n\n\n\nThe snippet below reflects the core logic [10].\nclass STEMFFN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 1. Gate Projection (W_g) - Kept Dense\n        # Determines \"how much\" of the knowledge to let through based on context.\n        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n\n        # 2. Down Projection (W_d) - Kept Dense\n        # Projects the result back to the model's hidden dimension.\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n\n        # 3. STEM Embedding Table (U_l) - THE REPLACEMENT\n        # Replaces the dense \"Up Projection\" (W_u).\n        # Size: [Vocab Size x FFN Intermediate Size]\n        # This is the \"Static Memory\" containing layer-local facts for each token.\n        self.stem_embedding = nn.Embedding(config.vocab_size, config.intermediate_size)\n\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x, input_ids):\n        # x shape: [Batch, Seq_Len, Hidden_Dim]\n        # input_ids shape: [Batch, Seq_Len]\n\n        # Step A: Compute the Gate (Contextual)\n        # Formula: SiLU(W_g * x)\n        # The gate looks at the *context* (x) to decide activation.\n        gate_output = self.act_fn(self.gate_proj(x))\n\n        # Step B: Retrieve Static Memory (Content)\n        # Formula: U_l[t]\n        # Instead of computing (W_u * x), we just look up the vector for the token.\n        # This vector represents the \"fact\" or \"value\" tied to this specific token in this layer.\n        stem_output = self.stem_embedding(input_ids)\n\n        # Step C: Combine (Gating)\n        # Formula: Gate * STEM_Embedding\n        # We multiply the static fact (stem_output) by the dynamic context (gate_output).\n        # If the gate is 0, the model ignores this fact for the current context.\n        activated_output = gate_output * stem_output\n\n        # Step D: Project Down\n        # Formula: W_d * (Activated_Output)\n        output = self.down_proj(activated_output)\n\n        return output\n\n\n\nThe STEM embeddings are inherently linked to individual tokens. Viewing this through a memory lens, this embedding is intended to consolidate the essential information tied to its corresponding token. Consequently, these embeddings can potentially serve as steering vectors.\n Figure: Knowledge injection/edit demonstration.\n\n\n\n\nTraining Stability: Unlike Mixture-of-Experts (MoE), which suffers from load-balancing issues and loss spikes, STEM trains as stably as dense models.\nEfficiency: Removes ~1/3 of FFN parameters from the active compute path.\nInterpretability (The “Killer Feature”): Because embeddings are tied to specific tokens (e.g., “Spain”), researchers can perform Knowledge Editing. By swapping the “Spain” embedding for “Germany” in specific layers, the model can be tricked into “hallucinating” consistent facts (e.g., saying the capital of Spain is Berlin) without changing the input text.\n\n\n\n\n\n\n\nDeepSeek introduces “Conditional Memory” [2] to address a fundamental inefficiency: while Transformers excel at reasoning (via Conditional Computation/MoE), they lack a native primitive for remembering, often wasting expensive compute to simulate knowledge retrieval.\n\nEngram Module: A dedicated module that augments the neural backbone, structurally separating the storage of static patterns from dynamic logic processing.\nModernized N-Grams: Instead of standard single-token lookups, the model uses Hashed N-Grams to map multi-token sequences (e.g., “The capital of”) to static embedding vectors via fast, constant-time \\(O(1)\\) retrieval.\nContext-Aware Gating: The retrieved memory is fused intelligently, not blindly. A gating mechanism uses the model’s current hidden state to evaluate the static fact, integrating it only when relevant to the context while suppressing noise.\n\n Figure: The Engram Architecture.\n\n\n\n\nThe U-Shaped Scaling Law: DeepSeek found an optimal ratio between “Thinking Parameters” (MoE) and “Remembering Parameters” (Engram). Allocating ~20% of the parameter budget to Engram yields better results than a pure MoE model.\nReasoning Gains: Surprisingly, offloading static knowledge to Engram improves reasoning benchmarks significantly. Engram-27B outperforms a parameter-equivalent MoE-27B baseline (e.g., BBH +5.0, MMLU +3.0, MATH +2.4). Why? By relieving the attention layers from the burden of “memorizing” facts, the model’s depth is freed up to perform complex logic.\nNeedle-in-a-Haystack: Massive gains in long-context retrieval (84.2% -&gt; 97.0%). The “local dependencies” are handled by the lookup table, leaving the attention mechanism free to scan the full global context.\nZero-Cost “Infinite” Memory: Engram decouples model size from inference latency. Because the lookups are deterministic (based on N-grams), the system can prefetch embeddings from CPU memory (RAM) before the GPU needs them. Result: Offloading a massive 100B-parameter Engram table to host memory incurs a negligible throughput penalty (&lt; 2.8%).\n\n Figure: Sparsity allocation and Engram scaling.\nUnlike previous works that simply demonstrate that external memory improves performance, this paper provides a rigorous ablation study. Through their “Sparsity Allocation” experiments, the authors reveal a fundamental U-shaped scaling law: purely scaling experts (MoE) or purely scaling memory (Engram) is suboptimal. Instead, they identify a precise “sweet spot”—allocating ~20-25% of sparse parameters to static memory minimizes validation loss.\n\n\n\n\n\n\nLongCat [5] challenges the dominant “Scaling Experts” (MoE) paradigm by proposing Embedding Scaling. Instead of adding more “brains” (experts) to the Feed-Forward Networks, it massively scales the Input Embedding Layer to capture rich local context.\n\nInput-Only Integration: Unlike STEM (which replaces FFNs) or Engram (which runs parallel to the backbone), LongCat adds N-gram embeddings only at the input level.\nMechanism: The model retrieves massive N-gram vectors (2-grams, 3-grams) and averages them with the standard token embedding to create a “super-charged” input vector \\(e_i\\).\nStandard Backbone: The rest of the Transformer (Attention & FFNs) remains unchanged. It simply processes these denser, richer input vectors.\n\n Figure: The architecture of a N-gram Embedding layer.\n\n\n\n\nThe “Fat Embedding” Pareto Frontier: The authors found that once the number of experts reaches a certain point, adding more memory (embeddings) yields better returns than adding more experts.\nThe 50% Allocation Rule: The optimal design allocates roughly 50% of the total parameter budget to these static embeddings. Example: LongCat-Flash-Lite is a 68.5B parameter model, but ~31.4B parameters are just the embedding table.\nWide vs. Deep: This method works best in wider models. In very deep models, the signal from the input embeddings tends to fade (“wash out”) as it propagates through many layers.\nPerformance: The 68.5B model (with only ~3B active params) outperformed parameter-equivalent MoE baselines, showing particular strength in coding and agentic tasks where local context is critical.\n\n\n\n\n\n\nAmeisen, E. et al. (2025). Circuit Tracing: Revealing Computational Graphs in Language Models. Anthropic.\nCheng, X. et al. (2026). Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models. arXiv:2601.07372.\nGoogle DeepMind (2025). Gemma 3 Technical Report. arXiv:2503.19786. Model card: Gemma 3n.\nLindsey, J. et al. (2025). On the Biology of a Large Language Model. Anthropic.\nLongCat (2026). Scaling Embeddings Outperforms Scaling Experts in Language Models. arXiv:2601.21204.\nSadhukhan, R. et al. (2026). STEM: Scaling Transformers with Embedding Modules. arXiv:2601.10639.\nTempleton, A. et al. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Anthropic.\nZhang, J. et al. (2025). On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models. arXiv:2512.07783.\nGoogle DeepMind. Gemma 3n model code. gemma3n/modeling_gemma3n.py.\nLingua / STEM. STEM model code. lingua/stem.py.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#background",
    "href": "posts/static_llm_memory/index.html#background",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Improving reasoning remains a primary goal for modern AI, yet recent findings (Zhang et al., 2025 [8]) highlight a critical bottleneck: a model’s reasoning potential is capped by the knowledge “primitives” absorbed during Pre-Training. While Post-Training can refine how a model thinks, it cannot fabricate knowledge from scratch; it merely optimizes the foundation laid earlier. Consequently, achieving substantial breakthroughs in reasoning requires massively scaling the pre-training stage to embed a far richer set of facts and primitives.\n\n\n\nStandard Transformers face a major inefficiency when scaling knowledge. Interpretability research from Anthropic [7, 1] reveals that dense models do not store information in a static database; instead, they dynamically “compute” it. Retrieving a simple concept like “The Golden Gate Bridge” triggers a complex, expensive sequence of feature interactions across layers. This architecture conflates storage with processing, forcing the model to burn valuable computational resources (FLOPs) just to “remember” rather than to “think”.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#overview-ple-stem-engram-longcat",
    "href": "posts/static_llm_memory/index.html#overview-ple-stem-engram-longcat",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Modern LLMs traditionally conflate “reasoning” (dynamic computation) with “knowledge” (static fact retrieval). We use the same expensive matrix multiplications to calculate math problems as we do to recall that “Paris is the capital of France”.\nFour emerging architectures—STEM (Meta/CMU) [6], PLE (Google DeepMind) [3], Engram (DeepSeek) [2], and LongCat (Meituan) [5]—address this by moving static information into Embedding Modules. Instead of computing facts, these models retrieve them. This shift decouples model capacity from inference cost, allowing for massive “memory” scaling without exploding GPU requirements.\n\n\nComparison of Static Memory Architectures\n\n\n\n\n\n\n\n\n\nFeature\nGemma 3n PLE (DeepMind)\nSTEM (Meta/CMU)\nEngram (DeepSeek)\nLongCat (Meituan)\n\n\n\n\nDate\nJune 26, 2025\nJan 15, 2026\nJan 12, 2026\nJan 29, 2026\n\n\nCore Concept\nAugmentation: Adds extra embeddings to modulate/augment layers.\nReplacement: Swaps FFN Up-Projection for a Lookup Table.\nComplement: Adds a “Memory Module” alongside the Neural Backbone.\nInput Augmentation: Expands the model’s vocabulary/input capacity rather than depth.\n\n\nMechanism\nPer-Layer Embeddings (Streamed)\nToken-indexed Lookup (Static)\nHashed N-Gram Lookup (Contextual)\nInput-Level N-Gram Lookup: Averages N-gram vectors into the input.\n\n\nHardware Goal\nOn-Device Efficiency (Low VRAM)\nTraining Stability & Interpretability\nMassive Scaling (Memory vs. Logic)\nCache Efficiency: Uses “N-gram Caching” to bypass communication bottlenecks.\n\n\nKey Win\nRunning 8B models on 3GB RAM phones\n“Surgical” Knowledge Editing\n+5.0 on Reasoning (BBH) by offloading facts\nPareto Efficiency: Beats parameter-equivalent MoEs on Coding & Agentic tasks.\n\n\nModel Size\n2B (Effective 4B)\n1B (Evaluated at 350M & 1B)\n40B (Tested up to 100B Engram module)\n68.5B (~31B of which are static N-gram embeddings)",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#gemma-3n-per-layer-embeddings",
    "href": "posts/static_llm_memory/index.html#gemma-3n-per-layer-embeddings",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Gemma 3n is engineered specifically for mobile/edge deployment [3]. Its goal is to fit a model with “8B parameter intelligence” onto a device that only has RAM for a 2B model.\n\nPLE (Per Layer Embeddings): Instead of storing all weights in the GPU/NPU memory (VRAM), it keeps massive embedding tables in the slower System RAM (CPU).\nStreaming: As the neural network processes layer \\(i\\), the specific embedding for that layer is streamed from the CPU to the NPU just in time.\nMechanism: PLEs do not usually replace the FFN. Instead, they modulate the residual stream or the FFN output. The STEM paper notes that PLEs are often much lower dimension (e.g., 256 dim) compared to the model’s width.\n\n\n\n\nThe Per-Layer Embedding (PLE) mechanism works by retrieving a massive, token-specific embedding vector that is pre-sliced to provide a unique “knowledge” input for every decoder layer [9]. As the model processes a token, each layer accesses its specific slice and dynamically gates it using the current hidden state—essentially using the active context to determine how much of the static retrieved knowledge to admit.\n# The massive \"Memory Bank\" storing all layer-specific vectors for every token.\n# Total Size: Vocab_PLE x (Num_Layers * PLE_Dim)\n# Example Dimensions: 2M tokens x (40 layers * 256 dim)\nself.embed_tokens_per_layer = Gemma3nTextScaledWordEmbedding(\n    config.vocab_size_per_layer_input,\n    config.num_hidden_layers * config.hidden_size_per_layer_input,\n    self.padding_idx,\n    embed_scale=config.hidden_size_per_layer_input**0.5,\n)\n\ndef get_per_layer_inputs(self, input_ids: torch.LongTensor) -&gt; torch.Tensor:\n    # Look up the massive vector and \"slice\" it for each layer.\n    # Input:  [Batch, Seq_Len]\n    # Output: [Batch, Seq_Len, Num_Layers, PLE_Dim]\n    return self.embed_tokens_per_layer(input_ids).reshape(\n        *input_ids.shape,\n        self.config.num_hidden_layers,\n        self.hidden_size_per_layer_input,\n    )\n\n# Inside the decoder loop, we pass the specific slice for this layer:\n# per_layer_input = per_layer_inputs[:, :, layer_idx, :] \n# Shape: [Batch, Seq_Len, PLE_Dim]\n\n# ... inside the layer's forward pass ...\n\n# A. GATE: Use the current context (hidden state) to determine \"how much\" memory to read.\n# Project from Model_Dim (e.g., 2560) -&gt; PLE_Dim (e.g., 256)\nfirst_prediction = self.per_layer_input_gate(first_prediction)\nfirst_prediction = self.act_fn(first_prediction)\n\n# B. MODULATE: Inject the retrieved static memory via element-wise multiplication.\n# Dynamic Context (Gate) * Static Memory (PLE Vector)\nfirst_prediction = torch.multiply(first_prediction, per_layer_input)\n\n# C. PROJECT: Mix the result back into the main residual stream.\n# Project from PLE_Dim (e.g., 256) -&gt; Model_Dim (e.g., 2560)\nfirst_prediction = self.per_layer_projection(first_prediction)\nfirst_prediction = self.post_per_layer_input_norm(first_prediction)\n\n# D. ADD: Add to the model's prediction stream\ncorrected_predictions[1:] += first_prediction\n\n\n\n\nVRAM Breakthrough: Allows running high-capacity models on phones with 3-4GB of RAM.\nParameters vs. Compute: It massively increases the parameter count (knowledge capacity) without increasing the FLOPs (compute cost) or the active VRAM usage.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#stem-scaling-transformers-with-embedding-modules",
    "href": "posts/static_llm_memory/index.html#stem-scaling-transformers-with-embedding-modules",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "STEM [6] identifies that the Up-Projection matrix in a Feed-Forward Network (FFN) acts largely as a “Key” lookup in a Key-Value memory system.\n\nChange: It replaces the dense Up-Projection matrix (\\(W^u\\)) with a static, token-indexed embedding table (\\(U_{token_id}\\)).\nRetention: It keeps the Gate Projection (\\(W^g\\)) and Down Projection (\\(W^d\\)) as dense layers.\nFormula: \\(y = W_{down}(\\text{SiLU}(W_{gate}x) \\odot \\text{Lookup}(TokenID))\\)\n\n Figure: Schematics of (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM.\n\n\n\nThe snippet below reflects the core logic [10].\nclass STEMFFN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 1. Gate Projection (W_g) - Kept Dense\n        # Determines \"how much\" of the knowledge to let through based on context.\n        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n\n        # 2. Down Projection (W_d) - Kept Dense\n        # Projects the result back to the model's hidden dimension.\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n\n        # 3. STEM Embedding Table (U_l) - THE REPLACEMENT\n        # Replaces the dense \"Up Projection\" (W_u).\n        # Size: [Vocab Size x FFN Intermediate Size]\n        # This is the \"Static Memory\" containing layer-local facts for each token.\n        self.stem_embedding = nn.Embedding(config.vocab_size, config.intermediate_size)\n\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x, input_ids):\n        # x shape: [Batch, Seq_Len, Hidden_Dim]\n        # input_ids shape: [Batch, Seq_Len]\n\n        # Step A: Compute the Gate (Contextual)\n        # Formula: SiLU(W_g * x)\n        # The gate looks at the *context* (x) to decide activation.\n        gate_output = self.act_fn(self.gate_proj(x))\n\n        # Step B: Retrieve Static Memory (Content)\n        # Formula: U_l[t]\n        # Instead of computing (W_u * x), we just look up the vector for the token.\n        # This vector represents the \"fact\" or \"value\" tied to this specific token in this layer.\n        stem_output = self.stem_embedding(input_ids)\n\n        # Step C: Combine (Gating)\n        # Formula: Gate * STEM_Embedding\n        # We multiply the static fact (stem_output) by the dynamic context (gate_output).\n        # If the gate is 0, the model ignores this fact for the current context.\n        activated_output = gate_output * stem_output\n\n        # Step D: Project Down\n        # Formula: W_d * (Activated_Output)\n        output = self.down_proj(activated_output)\n\n        return output\n\n\n\nThe STEM embeddings are inherently linked to individual tokens. Viewing this through a memory lens, this embedding is intended to consolidate the essential information tied to its corresponding token. Consequently, these embeddings can potentially serve as steering vectors.\n Figure: Knowledge injection/edit demonstration.\n\n\n\n\nTraining Stability: Unlike Mixture-of-Experts (MoE), which suffers from load-balancing issues and loss spikes, STEM trains as stably as dense models.\nEfficiency: Removes ~1/3 of FFN parameters from the active compute path.\nInterpretability (The “Killer Feature”): Because embeddings are tied to specific tokens (e.g., “Spain”), researchers can perform Knowledge Editing. By swapping the “Spain” embedding for “Germany” in specific layers, the model can be tricked into “hallucinating” consistent facts (e.g., saying the capital of Spain is Berlin) without changing the input text.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#engram-deepseek",
    "href": "posts/static_llm_memory/index.html#engram-deepseek",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "DeepSeek introduces “Conditional Memory” [2] to address a fundamental inefficiency: while Transformers excel at reasoning (via Conditional Computation/MoE), they lack a native primitive for remembering, often wasting expensive compute to simulate knowledge retrieval.\n\nEngram Module: A dedicated module that augments the neural backbone, structurally separating the storage of static patterns from dynamic logic processing.\nModernized N-Grams: Instead of standard single-token lookups, the model uses Hashed N-Grams to map multi-token sequences (e.g., “The capital of”) to static embedding vectors via fast, constant-time \\(O(1)\\) retrieval.\nContext-Aware Gating: The retrieved memory is fused intelligently, not blindly. A gating mechanism uses the model’s current hidden state to evaluate the static fact, integrating it only when relevant to the context while suppressing noise.\n\n Figure: The Engram Architecture.\n\n\n\n\nThe U-Shaped Scaling Law: DeepSeek found an optimal ratio between “Thinking Parameters” (MoE) and “Remembering Parameters” (Engram). Allocating ~20% of the parameter budget to Engram yields better results than a pure MoE model.\nReasoning Gains: Surprisingly, offloading static knowledge to Engram improves reasoning benchmarks significantly. Engram-27B outperforms a parameter-equivalent MoE-27B baseline (e.g., BBH +5.0, MMLU +3.0, MATH +2.4). Why? By relieving the attention layers from the burden of “memorizing” facts, the model’s depth is freed up to perform complex logic.\nNeedle-in-a-Haystack: Massive gains in long-context retrieval (84.2% -&gt; 97.0%). The “local dependencies” are handled by the lookup table, leaving the attention mechanism free to scan the full global context.\nZero-Cost “Infinite” Memory: Engram decouples model size from inference latency. Because the lookups are deterministic (based on N-grams), the system can prefetch embeddings from CPU memory (RAM) before the GPU needs them. Result: Offloading a massive 100B-parameter Engram table to host memory incurs a negligible throughput penalty (&lt; 2.8%).\n\n Figure: Sparsity allocation and Engram scaling.\nUnlike previous works that simply demonstrate that external memory improves performance, this paper provides a rigorous ablation study. Through their “Sparsity Allocation” experiments, the authors reveal a fundamental U-shaped scaling law: purely scaling experts (MoE) or purely scaling memory (Engram) is suboptimal. Instead, they identify a precise “sweet spot”—allocating ~20-25% of sparse parameters to static memory minimizes validation loss.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#longcat-scaling-embeddings-outperforms-scaling-experts",
    "href": "posts/static_llm_memory/index.html#longcat-scaling-embeddings-outperforms-scaling-experts",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "LongCat [5] challenges the dominant “Scaling Experts” (MoE) paradigm by proposing Embedding Scaling. Instead of adding more “brains” (experts) to the Feed-Forward Networks, it massively scales the Input Embedding Layer to capture rich local context.\n\nInput-Only Integration: Unlike STEM (which replaces FFNs) or Engram (which runs parallel to the backbone), LongCat adds N-gram embeddings only at the input level.\nMechanism: The model retrieves massive N-gram vectors (2-grams, 3-grams) and averages them with the standard token embedding to create a “super-charged” input vector \\(e_i\\).\nStandard Backbone: The rest of the Transformer (Attention & FFNs) remains unchanged. It simply processes these denser, richer input vectors.\n\n Figure: The architecture of a N-gram Embedding layer.\n\n\n\n\nThe “Fat Embedding” Pareto Frontier: The authors found that once the number of experts reaches a certain point, adding more memory (embeddings) yields better returns than adding more experts.\nThe 50% Allocation Rule: The optimal design allocates roughly 50% of the total parameter budget to these static embeddings. Example: LongCat-Flash-Lite is a 68.5B parameter model, but ~31.4B parameters are just the embedding table.\nWide vs. Deep: This method works best in wider models. In very deep models, the signal from the input embeddings tends to fade (“wash out”) as it propagates through many layers.\nPerformance: The 68.5B model (with only ~3B active params) outperformed parameter-equivalent MoE baselines, showing particular strength in coding and agentic tasks where local context is critical.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  },
  {
    "objectID": "posts/static_llm_memory/index.html#references",
    "href": "posts/static_llm_memory/index.html#references",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Ameisen, E. et al. (2025). Circuit Tracing: Revealing Computational Graphs in Language Models. Anthropic.\nCheng, X. et al. (2026). Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models. arXiv:2601.07372.\nGoogle DeepMind (2025). Gemma 3 Technical Report. arXiv:2503.19786. Model card: Gemma 3n.\nLindsey, J. et al. (2025). On the Biology of a Large Language Model. Anthropic.\nLongCat (2026). Scaling Embeddings Outperforms Scaling Experts in Language Models. arXiv:2601.21204.\nSadhukhan, R. et al. (2026). STEM: Scaling Transformers with Embedding Modules. arXiv:2601.10639.\nTempleton, A. et al. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Anthropic.\nZhang, J. et al. (2025). On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models. arXiv:2512.07783.\nGoogle DeepMind. Gemma 3n model code. gemma3n/modeling_gemma3n.py.\nLingua / STEM. STEM model code. lingua/stem.py.",
    "crumbs": [
      "Posts",
      "The Rise of Static Memory in LLMs"
    ]
  }
]