[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Brief Intro",
    "section": "",
    "text": "A Brief Intro\nI document hands-on explorations in LLM architecture and post-training. In an era of rapid research cycles, this log serves as a space to synthesize long-term trends and foundational modeling topics—specifically focusing on bridging deep research insights with the practical constraints of large-scale production systems.\n\n\nResearch Logs\n\n\n\n\n\n\n\n\n\n\nRethinking Supervised Fine-Tuning: A First-Principles Derivation\n\n\n\n\n\n\n\n\nFeb 17, 2026\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nThe Rise of Static Memory in LLMs\n\n\n\n\n\n\n\n\nFeb 1, 2026\n\n13 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/static_llm_memory/index.html",
    "href": "posts/static_llm_memory/index.html",
    "title": "The Rise of Static Memory in LLMs",
    "section": "",
    "text": "Topics: LLM Knowledge, Embedding Scaling, Sparse Architectures"
  },
  {
    "objectID": "posts/static_llm_memory/index.html#background",
    "href": "posts/static_llm_memory/index.html#background",
    "title": "The Rise of Static Memory in LLMs",
    "section": "1. Background",
    "text": "1. Background\n\nThe Need for Scale: Knowledge Primitives Limit Reasoning\nImproving reasoning remains a primary goal for modern AI, yet recent findings (Zhang et al., 2025 [8]) highlight a critical bottleneck: a model’s reasoning potential is capped by the knowledge “primitives” absorbed during Pre-Training. While Post-Training can refine how a model thinks, it cannot fabricate knowledge from scratch; it merely optimizes the foundation laid earlier. Consequently, achieving substantial breakthroughs in reasoning requires massively scaling the pre-training stage to embed a far richer set of facts and primitives.\n\n\nThe Representation Reality: Computing vs. Retrieving\nStandard Transformers face a major inefficiency when scaling knowledge. Interpretability research from Anthropic [7, 1] reveals that dense models do not store information in a static database; instead, they dynamically “compute” it. Retrieving a simple concept like “The Golden Gate Bridge” triggers a complex, expensive sequence of feature interactions across layers. This architecture conflates storage with processing, forcing the model to burn valuable computational resources (FLOPs) just to “remember” rather than to “think”."
  },
  {
    "objectID": "posts/static_llm_memory/index.html#overview-ple-stem-engram-longcat",
    "href": "posts/static_llm_memory/index.html#overview-ple-stem-engram-longcat",
    "title": "The Rise of Static Memory in LLMs",
    "section": "2. Overview: PLE, STEM, Engram, LongCat",
    "text": "2. Overview: PLE, STEM, Engram, LongCat\nModern LLMs traditionally conflate “reasoning” (dynamic computation) with “knowledge” (static fact retrieval). We use the same expensive matrix multiplications to calculate math problems as we do to recall that “Paris is the capital of France”.\nFour emerging architectures—STEM (Meta/CMU) [6], PLE (Google DeepMind) [3], Engram (DeepSeek) [2], and LongCat (Meituan) [5]—address this by moving static information into Embedding Modules. Instead of computing facts, these models retrieve them. This shift decouples model capacity from inference cost, allowing for massive “memory” scaling without exploding GPU requirements.\n\n\nComparison of Static Memory Architectures\n\n\n\n\n\n\n\n\n\nFeature\nGemma 3n PLE (DeepMind)\nSTEM (Meta/CMU)\nEngram (DeepSeek)\nLongCat (Meituan)\n\n\n\n\nDate\nJune 26, 2025\nJan 15, 2026\nJan 12, 2026\nJan 29, 2026\n\n\nCore Concept\nAugmentation: Adds extra embeddings to modulate/augment layers.\nReplacement: Swaps FFN Up-Projection for a Lookup Table.\nComplement: Adds a “Memory Module” alongside the Neural Backbone.\nInput Augmentation: Expands the model’s vocabulary/input capacity rather than depth.\n\n\nMechanism\nPer-Layer Embeddings (Streamed)\nToken-indexed Lookup (Static)\nHashed N-Gram Lookup (Contextual)\nInput-Level N-Gram Lookup: Averages N-gram vectors into the input.\n\n\nHardware Goal\nOn-Device Efficiency (Low VRAM)\nTraining Stability & Interpretability\nMassive Scaling (Memory vs. Logic)\nCache Efficiency: Uses “N-gram Caching” to bypass communication bottlenecks.\n\n\nKey Win\nRunning 8B models on 3GB RAM phones\n“Surgical” Knowledge Editing\n+5.0 on Reasoning (BBH) by offloading facts\nPareto Efficiency: Beats parameter-equivalent MoEs on Coding & Agentic tasks.\n\n\nModel Size\n2B (Effective 4B)\n1B (Evaluated at 350M & 1B)\n40B (Tested up to 100B Engram module)\n68.5B (~31B of which are static N-gram embeddings)"
  },
  {
    "objectID": "posts/static_llm_memory/index.html#gemma-3n-per-layer-embeddings",
    "href": "posts/static_llm_memory/index.html#gemma-3n-per-layer-embeddings",
    "title": "The Rise of Static Memory in LLMs",
    "section": "3. Gemma 3n (Per Layer Embeddings)",
    "text": "3. Gemma 3n (Per Layer Embeddings)\n\nThe Architecture\nGemma 3n is engineered specifically for mobile/edge deployment [3]. Its goal is to fit a model with “8B parameter intelligence” onto a device that only has RAM for a 2B model.\n\nPLE (Per Layer Embeddings): Instead of storing all weights in the GPU/NPU memory (VRAM), it keeps massive embedding tables in the slower System RAM (CPU).\nStreaming: As the neural network processes layer \\(i\\), the specific embedding for that layer is streamed from the CPU to the NPU just in time.\nMechanism: PLEs do not usually replace the FFN. Instead, they modulate the residual stream or the FFN output. The STEM paper notes that PLEs are often much lower dimension (e.g., 256 dim) compared to the model’s width.\n\n\n\nCore Model Code\nThe Per-Layer Embedding (PLE) mechanism works by retrieving a massive, token-specific embedding vector that is pre-sliced to provide a unique “knowledge” input for every decoder layer [9]. As the model processes a token, each layer accesses its specific slice and dynamically gates it using the current hidden state—essentially using the active context to determine how much of the static retrieved knowledge to admit.\n# The massive \"Memory Bank\" storing all layer-specific vectors for every token.\n# Total Size: Vocab_PLE x (Num_Layers * PLE_Dim)\n# Example Dimensions: 2M tokens x (40 layers * 256 dim)\nself.embed_tokens_per_layer = Gemma3nTextScaledWordEmbedding(\n    config.vocab_size_per_layer_input,\n    config.num_hidden_layers * config.hidden_size_per_layer_input,\n    self.padding_idx,\n    embed_scale=config.hidden_size_per_layer_input**0.5,\n)\n\ndef get_per_layer_inputs(self, input_ids: torch.LongTensor) -&gt; torch.Tensor:\n    # Look up the massive vector and \"slice\" it for each layer.\n    # Input:  [Batch, Seq_Len]\n    # Output: [Batch, Seq_Len, Num_Layers, PLE_Dim]\n    return self.embed_tokens_per_layer(input_ids).reshape(\n        *input_ids.shape,\n        self.config.num_hidden_layers,\n        self.hidden_size_per_layer_input,\n    )\n\n# Inside the decoder loop, we pass the specific slice for this layer:\n# per_layer_input = per_layer_inputs[:, :, layer_idx, :] \n# Shape: [Batch, Seq_Len, PLE_Dim]\n\n# ... inside the layer's forward pass ...\n\n# A. GATE: Use the current context (hidden state) to determine \"how much\" memory to read.\n# Project from Model_Dim (e.g., 2560) -&gt; PLE_Dim (e.g., 256)\nfirst_prediction = self.per_layer_input_gate(first_prediction)\nfirst_prediction = self.act_fn(first_prediction)\n\n# B. MODULATE: Inject the retrieved static memory via element-wise multiplication.\n# Dynamic Context (Gate) * Static Memory (PLE Vector)\nfirst_prediction = torch.multiply(first_prediction, per_layer_input)\n\n# C. PROJECT: Mix the result back into the main residual stream.\n# Project from PLE_Dim (e.g., 256) -&gt; Model_Dim (e.g., 2560)\nfirst_prediction = self.per_layer_projection(first_prediction)\nfirst_prediction = self.post_per_layer_input_norm(first_prediction)\n\n# D. ADD: Add to the model's prediction stream\ncorrected_predictions[1:] += first_prediction\n\n\nKey Findings\n\nVRAM Breakthrough: Allows running high-capacity models on phones with 3-4GB of RAM.\nParameters vs. Compute: It massively increases the parameter count (knowledge capacity) without increasing the FLOPs (compute cost) or the active VRAM usage."
  },
  {
    "objectID": "posts/static_llm_memory/index.html#stem-scaling-transformers-with-embedding-modules",
    "href": "posts/static_llm_memory/index.html#stem-scaling-transformers-with-embedding-modules",
    "title": "The Rise of Static Memory in LLMs",
    "section": "4. STEM (Scaling Transformers with Embedding Modules)",
    "text": "4. STEM (Scaling Transformers with Embedding Modules)\n\nThe Architecture\nSTEM [6] identifies that the Up-Projection matrix in a Feed-Forward Network (FFN) acts largely as a “Key” lookup in a Key-Value memory system.\n\nChange: It replaces the dense Up-Projection matrix (\\(W^u\\)) with a static, token-indexed embedding table (\\(U_{token_id}\\)).\nRetention: It keeps the Gate Projection (\\(W^g\\)) and Down Projection (\\(W^d\\)) as dense layers.\nFormula: \\(y = W_{down}(\\text{SiLU}(W_{gate}x) \\odot \\text{Lookup}(TokenID))\\)\n\n Figure: Schematics of (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM.\n\n\nCore Model Code\nThe snippet below reflects the core logic [10].\nclass STEMFFN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 1. Gate Projection (W_g) - Kept Dense\n        # Determines \"how much\" of the knowledge to let through based on context.\n        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n\n        # 2. Down Projection (W_d) - Kept Dense\n        # Projects the result back to the model's hidden dimension.\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n\n        # 3. STEM Embedding Table (U_l) - THE REPLACEMENT\n        # Replaces the dense \"Up Projection\" (W_u).\n        # Size: [Vocab Size x FFN Intermediate Size]\n        # This is the \"Static Memory\" containing layer-local facts for each token.\n        self.stem_embedding = nn.Embedding(config.vocab_size, config.intermediate_size)\n\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x, input_ids):\n        # x shape: [Batch, Seq_Len, Hidden_Dim]\n        # input_ids shape: [Batch, Seq_Len]\n\n        # Step A: Compute the Gate (Contextual)\n        # Formula: SiLU(W_g * x)\n        # The gate looks at the *context* (x) to decide activation.\n        gate_output = self.act_fn(self.gate_proj(x))\n\n        # Step B: Retrieve Static Memory (Content)\n        # Formula: U_l[t]\n        # Instead of computing (W_u * x), we just look up the vector for the token.\n        # This vector represents the \"fact\" or \"value\" tied to this specific token in this layer.\n        stem_output = self.stem_embedding(input_ids)\n\n        # Step C: Combine (Gating)\n        # Formula: Gate * STEM_Embedding\n        # We multiply the static fact (stem_output) by the dynamic context (gate_output).\n        # If the gate is 0, the model ignores this fact for the current context.\n        activated_output = gate_output * stem_output\n\n        # Step D: Project Down\n        # Formula: W_d * (Activated_Output)\n        output = self.down_proj(activated_output)\n\n        return output\n\n\nKnowledge Specificity & Interpretability\nThe STEM embeddings are inherently linked to individual tokens. Viewing this through a memory lens, this embedding is intended to consolidate the essential information tied to its corresponding token. Consequently, these embeddings can potentially serve as steering vectors.\n Figure: Knowledge injection/edit demonstration.\n\n\nKey Findings\n\nTraining Stability: Unlike Mixture-of-Experts (MoE), which suffers from load-balancing issues and loss spikes, STEM trains as stably as dense models.\nEfficiency: Removes ~1/3 of FFN parameters from the active compute path.\nInterpretability (The “Killer Feature”): Because embeddings are tied to specific tokens (e.g., “Spain”), researchers can perform Knowledge Editing. By swapping the “Spain” embedding for “Germany” in specific layers, the model can be tricked into “hallucinating” consistent facts (e.g., saying the capital of Spain is Berlin) without changing the input text."
  },
  {
    "objectID": "posts/static_llm_memory/index.html#engram-deepseek",
    "href": "posts/static_llm_memory/index.html#engram-deepseek",
    "title": "The Rise of Static Memory in LLMs",
    "section": "5. Engram (DeepSeek)",
    "text": "5. Engram (DeepSeek)\n\nThe Architecture\nDeepSeek introduces “Conditional Memory” [2] to address a fundamental inefficiency: while Transformers excel at reasoning (via Conditional Computation/MoE), they lack a native primitive for remembering, often wasting expensive compute to simulate knowledge retrieval.\n\nEngram Module: A dedicated module that augments the neural backbone, structurally separating the storage of static patterns from dynamic logic processing.\nModernized N-Grams: Instead of standard single-token lookups, the model uses Hashed N-Grams to map multi-token sequences (e.g., “The capital of”) to static embedding vectors via fast, constant-time \\(O(1)\\) retrieval.\nContext-Aware Gating: The retrieved memory is fused intelligently, not blindly. A gating mechanism uses the model’s current hidden state to evaluate the static fact, integrating it only when relevant to the context while suppressing noise.\n\n Figure: The Engram Architecture.\n\n\nKey Findings\n\nThe U-Shaped Scaling Law: DeepSeek found an optimal ratio between “Thinking Parameters” (MoE) and “Remembering Parameters” (Engram). Allocating ~20% of the parameter budget to Engram yields better results than a pure MoE model.\nReasoning Gains: Surprisingly, offloading static knowledge to Engram improves reasoning benchmarks significantly. Engram-27B outperforms a parameter-equivalent MoE-27B baseline (e.g., BBH +5.0, MMLU +3.0, MATH +2.4). Why? By relieving the attention layers from the burden of “memorizing” facts, the model’s depth is freed up to perform complex logic.\nNeedle-in-a-Haystack: Massive gains in long-context retrieval (84.2% -&gt; 97.0%). The “local dependencies” are handled by the lookup table, leaving the attention mechanism free to scan the full global context.\nZero-Cost “Infinite” Memory: Engram decouples model size from inference latency. Because the lookups are deterministic (based on N-grams), the system can prefetch embeddings from CPU memory (RAM) before the GPU needs them. Result: Offloading a massive 100B-parameter Engram table to host memory incurs a negligible throughput penalty (&lt; 2.8%).\n\n Figure: Sparsity allocation and Engram scaling.\nUnlike previous works that simply demonstrate that external memory improves performance, this paper provides a rigorous ablation study. Through their “Sparsity Allocation” experiments, the authors reveal a fundamental U-shaped scaling law: purely scaling experts (MoE) or purely scaling memory (Engram) is suboptimal. Instead, they identify a precise “sweet spot”—allocating ~20-25% of sparse parameters to static memory minimizes validation loss."
  },
  {
    "objectID": "posts/static_llm_memory/index.html#longcat-scaling-embeddings-outperforms-scaling-experts",
    "href": "posts/static_llm_memory/index.html#longcat-scaling-embeddings-outperforms-scaling-experts",
    "title": "The Rise of Static Memory in LLMs",
    "section": "6. LongCat (Scaling Embeddings Outperforms Scaling Experts)",
    "text": "6. LongCat (Scaling Embeddings Outperforms Scaling Experts)\n\nThe Architecture\nLongCat [5] challenges the dominant “Scaling Experts” (MoE) paradigm by proposing Embedding Scaling. Instead of adding more “brains” (experts) to the Feed-Forward Networks, it massively scales the Input Embedding Layer to capture rich local context.\n\nInput-Only Integration: Unlike STEM (which replaces FFNs) or Engram (which runs parallel to the backbone), LongCat adds N-gram embeddings only at the input level.\nMechanism: The model retrieves massive N-gram vectors (2-grams, 3-grams) and averages them with the standard token embedding to create a “super-charged” input vector \\(e_i\\).\nStandard Backbone: The rest of the Transformer (Attention & FFNs) remains unchanged. It simply processes these denser, richer input vectors.\n\n Figure: The architecture of a N-gram Embedding layer.\n\n\nKey Findings\n\nThe “Fat Embedding” Pareto Frontier: The authors found that once the number of experts reaches a certain point, adding more memory (embeddings) yields better returns than adding more experts.\nThe 50% Allocation Rule: The optimal design allocates roughly 50% of the total parameter budget to these static embeddings. Example: LongCat-Flash-Lite is a 68.5B parameter model, but ~31.4B parameters are just the embedding table.\nWide vs. Deep: This method works best in wider models. In very deep models, the signal from the input embeddings tends to fade (“wash out”) as it propagates through many layers.\nPerformance: The 68.5B model (with only ~3B active params) outperformed parameter-equivalent MoE baselines, showing particular strength in coding and agentic tasks where local context is critical."
  },
  {
    "objectID": "posts/static_llm_memory/index.html#references",
    "href": "posts/static_llm_memory/index.html#references",
    "title": "The Rise of Static Memory in LLMs",
    "section": "References",
    "text": "References\n\nAmeisen, E. et al. (2025). Circuit Tracing: Revealing Computational Graphs in Language Models. Anthropic.\nCheng, X. et al. (2026). Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models. arXiv:2601.07372.\nGoogle DeepMind (2025). Gemma 3 Technical Report. arXiv:2503.19786. Model card: Gemma 3n.\nLindsey, J. et al. (2025). On the Biology of a Large Language Model. Anthropic.\nLongCat (2026). Scaling Embeddings Outperforms Scaling Experts in Language Models. arXiv:2601.21204.\nSadhukhan, R. et al. (2026). STEM: Scaling Transformers with Embedding Modules. arXiv:2601.10639.\nTempleton, A. et al. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Anthropic.\nZhang, J. et al. (2025). On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models. arXiv:2512.07783.\nGoogle DeepMind. Gemma 3n model code. gemma3n/modeling_gemma3n.py.\nLingua / STEM. STEM model code. lingua/stem.py."
  },
  {
    "objectID": "posts/rethink_sft/index.html",
    "href": "posts/rethink_sft/index.html",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "",
    "text": "We typically view Supervised Fine-Tuning (SFT) as a process of addition: we feed a model new data to add new capabilities. But for anyone who has trained LLMs at scale, the reality feels more like subtraction. As we force a generalist model to mimic a narrow set of “gold” responses, we often trigger catastrophic forgetting, erasing the model’s subtle pre-trained priors in favor of rote memorization.\nWhy does this happen? This post argues that standard SFT is mathematically flawed for post-training. By treating fine-tuning as simple behavioral cloning on external datasets, we violate the fundamental “Trust Regions” required for stable learning.\nIn this deep dive, we will reconstruct the SFT objective from first principles. We will derive a unified framework, Contextual On-Policy Self-Distillation, which solves the forgetting problem by treating fine-tuning not as “learning from outsiders,” but as “learning from one’s better self.” We will show how recent state-of-the-art methods (from SDFT [5] to Thinking Machines [1]) are not different algorithms, but simply different “contexts” applied to this single, powerful mechanism."
  },
  {
    "objectID": "posts/rethink_sft/index.html#introduction-the-sft-paradox",
    "href": "posts/rethink_sft/index.html#introduction-the-sft-paradox",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "",
    "text": "We typically view Supervised Fine-Tuning (SFT) as a process of addition: we feed a model new data to add new capabilities. But for anyone who has trained LLMs at scale, the reality feels more like subtraction. As we force a generalist model to mimic a narrow set of “gold” responses, we often trigger catastrophic forgetting, erasing the model’s subtle pre-trained priors in favor of rote memorization.\nWhy does this happen? This post argues that standard SFT is mathematically flawed for post-training. By treating fine-tuning as simple behavioral cloning on external datasets, we violate the fundamental “Trust Regions” required for stable learning.\nIn this deep dive, we will reconstruct the SFT objective from first principles. We will derive a unified framework, Contextual On-Policy Self-Distillation, which solves the forgetting problem by treating fine-tuning not as “learning from outsiders,” but as “learning from one’s better self.” We will show how recent state-of-the-art methods (from SDFT [5] to Thinking Machines [1]) are not different algorithms, but simply different “contexts” applied to this single, powerful mechanism."
  },
  {
    "objectID": "posts/rethink_sft/index.html#the-mechanics-of-forgetting",
    "href": "posts/rethink_sft/index.html#the-mechanics-of-forgetting",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "2 The Mechanics of Forgetting",
    "text": "2 The Mechanics of Forgetting\nCapability regression during post-training or continual training, often termed “catastrophic forgetting”, is frequently treated as a mysterious side effect of fine-tuning, but it is better understood as a structural failure of the standard Supervised Fine-Tuning (SFT) objective. When we fine-tune a large language model, we are not merely adding a new skill; we are fundamentally altering its probability distribution.\n\n2.1 The Root Cause: Distributional Mismatch\nStandard SFT minimizes the negative log-likelihood of a static, external dataset. Mathematically, this forces the model to match a “demonstration distribution” that is often alien to its internal priors. Because the objective function contains no term to preserve the original model’s behavior, the optimization process is free to overwrite general capabilities (like coding, history, or logic) in its pursuit of minimizing loss on the specific fine-tuning tasks. The model “forgets” not because it runs out of space, but because the SFT objective explicitly rewards it for drifting away from its original manifold to fit the narrow distribution of the training set.\n\n\n2.2 RL’s Razor: The Mechanics of Forgetting\nRecent theoretical work has quantified the mechanics of this capability regression. The “RL’s Razor” paper [14] establishes a fundamental law of forgetting: the magnitude of capability loss is directly proportional to the KL divergence (distributional drift) between the fine-tuned model and its base version.\nThis highlights the critical structural flaw of standard Supervised Fine-Tuning (SFT). Because SFT is unconstrained—lacking any intrinsic mechanism to penalize distributional shift—it blindly pushes the model beyond the “trust region” where general capabilities reside [14].\nThe engine driving this drift is a capacity mismatch. As established by Allen-Zhu & Li (2024) [12], SFT is a high-bandwidth process capable of encoding roughly 2 bits of information per parameter, creating immense pressure for the model to memorize surface statistics. In contrast, as demonstrated by the efficiency of RL, the underlying objective of alignment is information-sparse (often ≈ 1 bit per episode) [13]. By using a high-capacity tool (SFT) to force-feed this sparse signal as if it were dense data, we induce the overfitting that RL’s Razor [14] identifies as the primary driver of capability regression."
  },
  {
    "objectID": "posts/rethink_sft/index.html#current-mitigations-and-their-limits",
    "href": "posts/rethink_sft/index.html#current-mitigations-and-their-limits",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "3 Current Mitigations and Their Limits",
    "text": "3 Current Mitigations and Their Limits\nThe industry currently relies on two primary strategies to combat this regression. While effective to a degree, they function as “band-aids” that address the symptoms of drift rather than the root cause of the objective mismatch.\n\n3.1 Data Replay (The “Brute Force” Approach)\nThe most common solution, exemplified by models like Llama 3, is data replay (or “generative replay”). This involves mixing a significant portion of general data (e.g., Wikipedia, code, web text) into the fine-tuning dataset.\n\nWhy it works: Theoretically, replay works by constraining the optimization landscape. As formalized in Linear Mode Connectivity [15], it forces the gradient descent process to find a solution that lies in the intersection of the low-loss basins for both the new task and the general domain. Furthermore, approaches like GEM [16] demonstrate that by treating general capabilities as active constraints (ensuring gradient alignment) rather than passive priors, replay prevents the model from moving into regions where “general intelligence” loss is high.\nThe “Provenance Gap” Limitation: Beyond computational inefficiency, the fatal flaw for most practitioners is data provenance. Post-training engineers almost never have access to the original, proprietary pre-training corpus (e.g., the exact 15T tokens used to train Llama 3). Instead, they must rely on “proxy replay” (public datasets like SlimPajama or Wikipedia). Because this proxy distribution differs from the model’s true internal priors, “replay” can ironically induce new distributional drift, overwriting the model’s actual knowledge with the proxy dataset’s quirks.\n\n\n\n3.2 Parameter-Efficient Fine-Tuning (The “Constraint” Approach)\nThe second common strategy is to use Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA (Low-Rank Adaptation), which freeze the majority of the model’s weights and only train a small subset of adapters.\n\nThe “Learn Less, Forget Less” Trade-off: As demonstrated in LoRA Learns Less and Forgets Less [17], PEFT does not magically solve forgetting; it merely shifts the model along the Pareto frontier. By restricting updates to a low-rank subspace, LoRA physically prevents the model from making the high-rank weight perturbations often required to overwrite existing knowledge. Consequently, it forgets less simply because it changes less.\nCapacity Starvation: The cost of this stability is a “glass ceiling” on adaptation. While LoRA Without Regret [17] demonstrates that LoRA can match full fine-tuning for reasoning and RL tasks (which require low-rank policy updates), it explicitly notes that LoRA underperforms in knowledge-intensive settings (like continual pre-training). If a task requires memorizing vast amounts of new information, e.g. a high-rank update, LoRA’s limited parameter budget prevents it from absorbing the data, effectively preventing the model from fully learning the target distribution."
  },
  {
    "objectID": "posts/rethink_sft/index.html#the-evolution-of-sft-deriving-contextual-on-policy-self-distillation",
    "href": "posts/rethink_sft/index.html#the-evolution-of-sft-deriving-contextual-on-policy-self-distillation",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "4 The Evolution of SFT: Deriving Contextual On-Policy Self-Distillation",
    "text": "4 The Evolution of SFT: Deriving Contextual On-Policy Self-Distillation\nTo solve the Forgetting Problem without sacrificing capacity or efficiency, we must reconstruct the SFT objective by addressing the failures identified in Section 1. This derivation unfolds as a progression of necessary constraints: first anchoring the model to prevent drift, then aligning the data source to the model’s own distribution, and finally introducing context to drive improvement.\n\n4.1 The Foundation: Anchoring via Trust Regions\nStandard SFT is technically unconstrained; it allows the model to drift arbitrarily far from its base distribution to minimize loss. The first theoretical fix is to enforce a Trust Region, effectively treating SFT as a constrained optimization problem.\n\nThe Theoretical Bridge: Recent work on Anchored SFT (ASFT) [11] demonstrates that SFT with a KL-divergence penalty is mathematically equivalent to offline Reinforcement Learning.\nWhy it works: By explicitly adding the KL term to the objective, we force the model to balance “learning the new task” with “staying close to the prior.” However, because ASFT is typically applied to static, external datasets, it remains an off-policy method, leaving it vulnerable to distribution mismatch.\n\n\n\n4.2 The Mechanism: On-Policy Distillation and Reverse KL\n\n\n\nFigure 1: The theoretical justification for Reverse KL (from MiniLLM [18]). Standard SFT (Forward KL) forces the model to cover the teacher’s entire distribution, leading to probability mass in “void regions” (hallucination). On-Policy Distillation (Reverse KL) is mode-seeking, forcing the model to focus only on high-confidence regions.\n\n\nTo strictly avoid the “alien data” problem, we must train on the model’s own samples. This necessitates a fundamental shift in the training objective from standard Cross-Entropy (Forward KL) to Reverse KL, as established in MiniLLM [18].\n\nMinimizing Exposure Bias: Standard SFT suffers from exposure bias because the model is trained on “gold” teacher trajectories (\\(p_{data}\\)) but must generate its own trajectories during inference (\\(p_{\\theta}\\)). As errors accumulate, the model drifts into states it has never seen. By training on its own rollouts (\\(x \\sim p_{\\theta}\\)), on-policy distillation forces the model to learn how to recover from its own specific deviations, ensuring the training distribution matches the inference reality.\nThe Shift to Reverse KL (Precision over Hedging): The move to on-policy data requires inverting the divergence measure, which fundamentally alters the model’s behavior:\n\nForward KL (SFT Standard): Minimizes \\(D_{KL}(p_{data} || p_{\\theta})\\). This is mean-seeking, forcing the student to cover the teacher’s entire distribution. As argued in MiniLLM [18], if the student lacks the capacity to model the teacher’s full complexity (especially the “long tail”), it is forced to “hedge” its bets. It assigns probability mass to unlikely or noisy tokens (“void regions”) just to avoid zero-probability penalties, directly leading to hallucinations and generic responses.\nReverse KL (On-Policy Standard): Minimizes \\(D_{KL}(p_{\\theta} || p_{teacher})\\). This is mode-seeking (or “zero-forcing”). It heavily penalizes the student for generating anything the teacher considers incorrect but does not penalize it for missing the teacher’s long tail. This encourages the student to collapse its probability mass solely onto the teacher’s highest-confidence modes, resulting in more precise and factually accurate generations.\n\nEmpirical Validation (Reversibility): The power of this mechanism is validated by the fact that “forgetting” is often reversible. A compelling example is Thinking Machines Lab’s Training an Internal Assistant study [1]. When a model was fine-tuned on internal docs, it lost its instruction-following ability. However, by using the original model as a teacher to distill behavior back into the fine-tuned version, the capability was restored. This confirms that capability regression arises from objective mismatch—which on-policy distillation effectively resolves—rather than a permanent loss of capacity.\nImplementation Note: In practice, pure Reverse KL can lead to excessive mode collapse (lack of diversity). State-of-the-art approaches like Generalized Knowledge Distillation (GKD) [6] often utilize Jensen-Shannon Divergence (JSD)—a symmetric, bounded mixture of Forward and Reverse KL—to stabilize training while retaining the on-policy benefits of the reverse objective.\n\n\n\n4.3 The Synthesis: Contextual On-Policy Self-Distillation\nThis creates a paradox: if the model minimizes Reverse KL against its own unguided distribution, it collapses into a degenerate state. To enable learning, we need a target that is internal (to maintain the on-policy manifold) yet superior (to provide a learning signal).\nThe solution is Contextual On-Policy Self-Distillation. We split the model into two states:\n\nThe Student (\\(p_{\\theta}\\)): The standard model (unconditioned), generating the on-policy rollout.\nThe Teacher (\\(p_{\\theta}(\\cdot|C)\\)): The same model, conditioned on Context \\(C\\) (e.g., retrieval, reasoning chains, or safety specs).\n\nThe Student minimizes the divergence (Reverse KL or JSD) between its own unconditioned output and the Teacher’s context-enhanced output. This ensures the update is strictly On-Policy (solving exposure bias) and Anchored (minimizing drift), while the Context provides the necessary gradient for capability improvement."
  },
  {
    "objectID": "posts/rethink_sft/index.html#a-taxonomy-of-contextual-self-distillation",
    "href": "posts/rethink_sft/index.html#a-taxonomy-of-contextual-self-distillation",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "5 A Taxonomy of Contextual Self-Distillation",
    "text": "5 A Taxonomy of Contextual Self-Distillation\nWe can now formally define the solution. Across various domains, state-of-the-art methods share the same underlying structure: they all minimize the Reverse KL divergence between the student’s unconditioned policy (\\(p_\\theta\\)) and a context-enhanced teacher (\\(p_\\theta(\\cdot|C)\\)):\n\\[\\mathcal{L} = D_{KL}( p_\\theta(y|x) \\parallel p_\\theta(y|x, C) )\\]\nThese approaches are not different algorithms; they are merely distinct choices of Context (\\(C\\)) used to drive the self-distillation process. To avoid confusion, we categorize them by the Nature of the Context and the Learning Objective.\n\n5.1 The Comparison Matrix\n\n\n\n\n\n\n\n\n\n\nType\nContext (\\(C\\))\nTeacher’s Advantage\nStudent’s Goal\nPrimary Use Case\n\n\n\n\nType A (SDFT)\nExamples (Few-Shot)\n“I see how to do it.”\nMaintenance (Don’t forget)\nContinual Learning\n\n\nType B (Deliberative)\nRules (Safety Specs)\n“I check the rules.”\nAlignment (Be safe instinctively)\nSafety / Constitutional AI\n\n\nType C (OPCD)\nInstructions (System Prompt)\n“I am told who to be.”\nEfficiency (Save tokens)\nPrompt Engineering Compilation\n\n\nType D (PID)\nOmniscience (Answer Key)\n“I know the answer.”\nReasoning (Derive the path)\nComplex Reasoning (Math/Code)\n\n\n\n\n\n5.2 Type A: Context = Demonstrations (SDFT)\n\n\n\nFigure 2: Empirical validation of on-policy distillation for maintenance (from SDFT [5]). While standard SFT leads to catastrophic forgetting on previous tasks (bottom), Self-Distillation (top) allows the model to “compile” new demonstrations into weights without degrading existing capabilities.\n\n\nIn Self-Distillation Fine-Tuning (SDFT) [5], the goal is to prevent capability regression during continual learning. Here, the context \\(C\\) is a set of few-shot expert demonstrations. The model uses its latent capacity to perform the task in-context (the Teacher state). SDFT then “compiles” this transient, prompt-dependent ability into the permanent weights of the Student. Because the student mimics its own context-enhanced outputs, the gradient update remains strictly on-policy, minimizing drift while locking in the new skill.\n\n\n5.3 Type B: Context = Safety Specifications (Deliberative Alignment)\n\n\n\nFigure 3: The mechanics of Deliberative Alignment (from Guan et al. [9]). The Teacher uses the external safety policy (Context) to generate a “deliberative” Chain-of-Thought. The Student distills this reasoning process, learning to align with safety rules instinctively.\n\n\nIn approaches like Deliberative Alignment [9], the goal is to internalize complex safety rules. The context \\(C\\) is the full Safety Policy or Specification. The Teacher generates synthetic data where it explicitly “deliberates” on these rules (System 2 thinking) before answering. The Student then distills this process, effectively internalizing the external policy document into its weights. The result is a model that adheres to the safety rules “intuitively” (System 1) without needing the policy document present at inference time.\n\n\n5.4 Type C: Context = System Prompts (OPCD)\n\n\n\nFigure 4: The unified architecture of Contextual On-Policy Self-Distillation (from OPCD [7]). The Student generates the trajectory (\\(y \\sim \\pi_\\theta\\)), ensuring the update is on-policy. The Teacher scores this trajectory conditioned on the context (\\(C\\)), providing the learning signal.\n\n\nIn On-Policy Context Distillation (OPCD) [7], the goal is to internalize instructions or past experiences. The context \\(C\\) consists of System Prompts or Historical Solution Traces. Unlike standard instruction tuning (which is off-policy), OPCD trains the student to match the distribution of the prompted teacher using the student’s own rollouts. This allows the model to “absorb” the system prompt into its weights, behaving as if it were prompted without incurring the inference cost of processing the prompt tokens.\n\n\n5.5 Type D: Context = Privileged Information (PID / OPSD)\nIn Privileged Information Distillation [8], the goal is to solve hard reasoning tasks. The context \\(C\\) is Privileged Information (PI)—such as ground truth answers, future states, or hidden “thoughts”—that acts as a cheat sheet. The paper introduces \\(\\pi\\)-Distill and OPSD (On-Policy Self-Distillation) to handle this. The Teacher sees the future/answer and guides the blind Student. The Student minimizes the divergence from this “omniscient” teacher, effectively learning to simulate the latent reasoning process required to reach the correct solution without actually seeing the cheat sheet."
  },
  {
    "objectID": "posts/rethink_sft/index.html#conclusion-the-perfect-pre-rl-state",
    "href": "posts/rethink_sft/index.html#conclusion-the-perfect-pre-rl-state",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "6 Conclusion: The Perfect Pre-RL State",
    "text": "6 Conclusion: The Perfect Pre-RL State\nWe started with a simple question: How do we stop SFT from breaking the model?\nThe answer, it turns out, is to stop treating SFT and RL as opposites. By applying RL’s Razor [14] (the “Trust Region” constraint), we found that the only safe way to fine-tune a model is to use its own distribution as the training data (On-Policy) and its own priors as the anchor (Reverse KL).\nThe Unified Picture\nThis framework gives us a new way to categorize the explosion of post-training papers. They aren’t inventing new physics; they are just choosing different Contexts to guide the self-distillation process:\n\nWant to maintain skills? Use Examples (Type A) [5].\nWant to align behavior? Use Safety Specs (Type B) [9].\nWant to save tokens? Use Instructions (Type C) [7].\nWant to improve reasoning? Use Privileged Info (Type D) [8].\n\nThe Takeaway\nThis suggests that the future of post-training isn’t about collecting massive external datasets, but about designing better Contexts. If we can prompt the model to be smart once (the Teacher), we can distill that intelligence forever (the Student). This approach yields the “Perfect Pre-RL State”: a model that is already aligned, robust, and mathematically anchored—ready to be further optimized by Reinforcement Learning without the risk of collapse."
  },
  {
    "objectID": "posts/rethink_sft/index.html#references",
    "href": "posts/rethink_sft/index.html#references",
    "title": "Rethinking Supervised Fine-Tuning: A First-Principles Derivation",
    "section": "7 References",
    "text": "7 References\n\nOn-Policy Distillation. Thinking Machines Lab. 2025.\nRetaining by Doing: The Role of On-Policy Data in Mitigating Forgetting. Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen. 2025.\nAgentic Continual Pre-Training. Tongyi Lab DeepResearch. 2025.\nThe LLaMA-3 Technical Report. Meta AI. 2024.\nSelf-Distillation Enables Continual Learning. Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal. 2026.\nOn-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem. 2023.\nOn-Policy Context Distillation for Language Models. Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei. 2026.\nPrivileged Information Distillation for Language Models. Emiliano Penaloza, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin. 2026.\nDeliberative Alignment: Reasoning Enables Safer Language Models. Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, et al. 2025.\nOn the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification. Yongliang Wu, Yizhou Zhou, Ziheng Zhou, Yingzhe Peng, Xinyu Ye, et al. 2025.\nAnchored Supervised Fine-Tuning. He Zhu, Junyou Su, Peng Lai, Ren Ma, Wenjia Zhang, Linyi Yang, Guanhua Chen. 2026.\nPhysics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws. Zeyuan Allen-Zhu, Yuanzhi Li. 2024.\nLoRA Without Regret. Thinking Machines Lab (John Schulman). 2025.\nRL’s Razor. Idan Shenfeld, Jyothish Pari, Pulkit Agrawal. 2025.\nLinear Mode Connectivity in Multitask and Continual Learning. Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, Hassan Ghasemzadeh. 2021.\nGradient Episodic Memory for Continual Learning. David Lopez-Paz, Marc’Aurelio Ranzato. 2017.\nLoRA Learns Less and Forgets Less. Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, et al. 2024.\nMiniLLM: Knowledge Distillation of Large Language Models. Yuxian Gu, Li Dong, Furu Wei, Minlie Huang. 2023."
  }
]