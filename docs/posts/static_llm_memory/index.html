<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Zhoutong Fu">
<meta name="dcterms.date" content="2025-01-26">
<meta name="description" content="How STEM, Gemma 3n (PLE), and DeepSeek Engram are decoupling knowledge from reasoning.">

<title>The Rise of Static Memory in LLMs – Zhoutong | AI Research Log</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Zhoutong | AI Research Log</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhoutongfu/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zhoutongfu"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">Posts</a></li><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">The Rise of Static Memory in LLMs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/static_llm_memory/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">The Rise of Static Memory in LLMs</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background-the-memory-compute-bottleneck" id="toc-background-the-memory-compute-bottleneck" class="nav-link active" data-scroll-target="#background-the-memory-compute-bottleneck">1. Background: The Memory-Compute Bottleneck</a>
  <ul class="collapse">
  <li><a href="#knowledge-primitives-limit-reasoning" id="toc-knowledge-primitives-limit-reasoning" class="nav-link" data-scroll-target="#knowledge-primitives-limit-reasoning">Knowledge Primitives Limit Reasoning</a></li>
  <li><a href="#the-cost-of-remembering" id="toc-the-cost-of-remembering" class="nav-link" data-scroll-target="#the-cost-of-remembering">The Cost of “Remembering”</a></li>
  </ul></li>
  <li><a href="#the-solution-static-memory-architectures" id="toc-the-solution-static-memory-architectures" class="nav-link" data-scroll-target="#the-solution-static-memory-architectures">2. The Solution: Static Memory Architectures</a></li>
  <li><a href="#gemma-3n-per-layer-embeddings" id="toc-gemma-3n-per-layer-embeddings" class="nav-link" data-scroll-target="#gemma-3n-per-layer-embeddings">3. Gemma 3n (Per Layer Embeddings)</a>
  <ul class="collapse">
  <li><a href="#the-architecture" id="toc-the-architecture" class="nav-link" data-scroll-target="#the-architecture">The Architecture</a></li>
  <li><a href="#core-implementation" id="toc-core-implementation" class="nav-link" data-scroll-target="#core-implementation">Core Implementation</a></li>
  </ul></li>
  <li><a href="#stem-scaling-transformers-with-embedding-modules" id="toc-stem-scaling-transformers-with-embedding-modules" class="nav-link" data-scroll-target="#stem-scaling-transformers-with-embedding-modules">4. STEM (Scaling Transformers with Embedding Modules)</a>
  <ul class="collapse">
  <li><a href="#the-architecture-1" id="toc-the-architecture-1" class="nav-link" data-scroll-target="#the-architecture-1">The Architecture</a></li>
  <li><a href="#key-benefits" id="toc-key-benefits" class="nav-link" data-scroll-target="#key-benefits">Key Benefits</a></li>
  </ul></li>
  <li><a href="#deepseek-engram" id="toc-deepseek-engram" class="nav-link" data-scroll-target="#deepseek-engram">5. DeepSeek Engram</a>
  <ul class="collapse">
  <li><a href="#the-architecture-2" id="toc-the-architecture-2" class="nav-link" data-scroll-target="#the-architecture-2">The Architecture</a></li>
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">Posts</a></li><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">The Rise of Static Memory in LLMs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">The Rise of Static Memory in LLMs</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLMs</div>
    <div class="quarto-category">Architecture</div>
    <div class="quarto-category">Memory</div>
  </div>
  </div>

<div>
  <div class="description">
    How STEM, Gemma 3n (PLE), and DeepSeek Engram are decoupling knowledge from reasoning.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Zhoutong Fu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background-the-memory-compute-bottleneck" class="level2">
<h2 class="anchored" data-anchor-id="background-the-memory-compute-bottleneck">1. Background: The Memory-Compute Bottleneck</h2>
<section id="knowledge-primitives-limit-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-primitives-limit-reasoning">Knowledge Primitives Limit Reasoning</h3>
<p>Improving reasoning is a core objective of modern AI. However, recent research suggests a critical limit: a model’s ultimate reasoning capacity is fixed by the fundamental knowledge “primitives” acquired during Pre-Training. While Post-Training strategies like Reinforcement Learning (RL) can enhance skills, they cannot create knowledge from scratch; they can only build upon the latent foundation laid earlier <em>(Zhang et al., 2025)</em>.</p>
</section>
<section id="the-cost-of-remembering" class="level3">
<h3 class="anchored" data-anchor-id="the-cost-of-remembering">The Cost of “Remembering”</h3>
<p>Standard Transformers face a computational inefficiency challenge when scaling this knowledge foundation. Research on interpretability indicates that dense models do not retain knowledge in a static database; instead, they dynamically “compute” it <em>(Templeton et al., 2024; Ameisen et al., 2025)</em>.</p>
<p>Even a simple recall task, such as retrieving “The Golden Gate Bridge,” necessitates a costly, multi-step sequence of feature interactions across the model’s layers. This design leads to a “Memory-Compute” bottleneck: we use the same expensive matrix multiplications to calculate complex math problems as we do to recall that “Paris is the capital of France.”</p>
<p>To achieve further scalability, we need architectures that separate the functions of “knowing” from “thinking” <em>(Lindsey et al., 2025)</em>.</p>
</section>
</section>
<section id="the-solution-static-memory-architectures" class="level2">
<h2 class="anchored" data-anchor-id="the-solution-static-memory-architectures">2. The Solution: Static Memory Architectures</h2>
<p>Three new architectures—<strong>STEM</strong> (Meta/CMU), <strong>PLE</strong> (Google DeepMind), and <strong>Engram</strong> (DeepSeek)—address this by moving static information into Embedding Modules. Instead of <em>computing</em> facts, these models <em>retrieve</em> them.</p>
<table class="caption-top table">
<caption>Comparison of Static Memory Architectures</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Gemma 3n PLE <em>(DeepMind)</em></th>
<th style="text-align: left;">STEM <em>(Meta/CMU)</em></th>
<th style="text-align: left;">Engram <em>(DeepSeek)</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Core Concept</strong></td>
<td style="text-align: left;"><strong>Augmentation:</strong> Adds extra embeddings to modulate layers.</td>
<td style="text-align: left;"><strong>Replacement:</strong> Swaps FFN Up-Projection for a Lookup Table.</td>
<td style="text-align: left;"><strong>Complement:</strong> Adds a “Memory Module” alongside the backbone.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Mechanism</strong></td>
<td style="text-align: left;">Per-Layer Embeddings (Streamed)</td>
<td style="text-align: left;">Token-indexed Lookup (Static)</td>
<td style="text-align: left;">Hashed N-Gram Lookup (Contextual)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Hardware Goal</strong></td>
<td style="text-align: left;">On-Device Efficiency (Low VRAM)</td>
<td style="text-align: left;">Training Stability &amp; Interpretability</td>
<td style="text-align: left;">Massive Scaling (Memory vs.&nbsp;Logic)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Key Win</strong></td>
<td style="text-align: left;">Running 8B models on 3GB RAM phones</td>
<td style="text-align: left;">“Surgical” Knowledge Editing</td>
<td style="text-align: left;">+5.0 on Reasoning (BBH)</td>
</tr>
</tbody>
</table>
</section>
<section id="gemma-3n-per-layer-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="gemma-3n-per-layer-embeddings">3. Gemma 3n (Per Layer Embeddings)</h2>
<p><strong>Ref:</strong> <em>Gemma 3n Technical Report (2025)</em></p>
<p>Gemma 3n is engineered specifically for mobile/edge deployment. Its goal is to fit a model with “8B parameter intelligence” onto a device that only has RAM for a 2B model.</p>
<section id="the-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture">The Architecture</h3>
<ul>
<li><strong>PLE (Per Layer Embeddings):</strong> Instead of storing all weights in VRAM, it keeps massive embedding tables in the slower System RAM (CPU).</li>
<li><strong>Streaming:</strong> As the neural network processes layer <span class="math inline">\(i\)</span>, the specific embedding for that layer is streamed from the CPU to the NPU just in time.</li>
<li><strong>Modulation:</strong> PLEs do not replace the FFN; they modulate the residual stream or FFN output.</li>
</ul>
</section>
<section id="core-implementation" class="level3">
<h3 class="anchored" data-anchor-id="core-implementation">Core Implementation</h3>
<p>The mechanism works by retrieving a massive, token-specific embedding vector pre-sliced for every decoder layer.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conceptual implementation based on Gemma 3n</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Ref: gemma3n/modeling_gemma3n.py</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Gemma3nModel(nn.Module):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The massive "Memory Bank" storing all layer-specific vectors for every token.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total Size: Vocab_PLE x (Num_Layers * PLE_Dim)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_tokens_per_layer <span class="op">=</span> Gemma3nTextScaledWordEmbedding(</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            config.vocab_size_per_layer_input,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            config.num_hidden_layers <span class="op">*</span> config.hidden_size_per_layer_input,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.padding_idx,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            embed_scale<span class="op">=</span>config.hidden_size_per_layer_input <span class="op">*</span> <span class="fl">0.5</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_per_layer_inputs(<span class="va">self</span>, input_ids: torch.LongTensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look up the massive vector and "slice" it for each layer.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output: [Batch, Seq_Len, Num_Layers, PLE_Dim]</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.embed_tokens_per_layer(input_ids).reshape(</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>input_ids.shape,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.config.num_hidden_layers,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_size_per_layer_input,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inside the decoder layer forward pass:</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_layer(<span class="va">self</span>, first_prediction, per_layer_input):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A. GATE: Use context to determine "how much" memory to read.</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        first_prediction <span class="op">=</span> <span class="va">self</span>.per_layer_input_gate(first_prediction)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        first_prediction <span class="op">=</span> <span class="va">self</span>.act_fn(first_prediction)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B. MODULATE: Inject retrieved static memory via element-wise multiplication.</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        first_prediction <span class="op">=</span> torch.multiply(first_prediction, per_layer_input)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># C. PROJECT: Mix result back into main residual stream.</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        first_prediction <span class="op">=</span> <span class="va">self</span>.per_layer_projection(first_prediction)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        first_prediction <span class="op">=</span> <span class="va">self</span>.post_per_layer_input_norm(first_prediction)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># D. ADD</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> first_prediction</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="stem-scaling-transformers-with-embedding-modules" class="level2">
<h2 class="anchored" data-anchor-id="stem-scaling-transformers-with-embedding-modules">4. STEM (Scaling Transformers with Embedding Modules)</h2>
<p><strong>Ref:</strong> <em>He et al.&nbsp;(2026)</em></p>
<p>STEM identifies that the Up-Projection matrix in a Feed-Forward Network (FFN) acts largely as a “Key” lookup in a Key-Value memory system.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/stem_schematic.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Schematics comparing (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM with a single prefetched token embedding.</figcaption>
</figure>
</div>
<section id="the-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-1">The Architecture</h3>
<p>STEM replaces the dense Up-Projection matrix (<span class="math inline">\(W^u\)</span>) with a static, token-indexed embedding table (<span class="math inline">\(U_{token\_id}\)</span>). It retains the Gate and Down projections as dense layers.</p>
<p><span class="math display">\[ y = W_{down}(\text{SiLU}(W_{gate}x) \odot \text{Lookup}(TokenID)) \]</span></p>
</section>
<section id="key-benefits" class="level3">
<h3 class="anchored" data-anchor-id="key-benefits">Key Benefits</h3>
<ul>
<li><strong>Interpretability:</strong> Because embeddings are linked to individual tokens, researchers can perform “Knowledge Editing”. By swapping the “Spain” embedding for “Germany” in specific layers, the model can be tricked into “hallucinating” consistent facts (e.g., saying the capital of Spain is Berlin) without changing the input text.</li>
<li><strong>Efficiency:</strong> It removes approximately 1/3 of the FFN parameters from the active compute path.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/stem_knowledge_edit.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Knowledge injection/edit demonstration. Swapping the PLE for ‘Spain’ with ‘Germany’ flips the generated capital from Madrid to Berlin.</figcaption>
</figure>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ref: lingua/stem.py</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> STEMFFN(nn.Module):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Gate Projection (W_g) - Kept Dense</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gate_proj <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Down Projection (W_d) - Kept Dense</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_proj <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. STEM Embedding Table (U_l) - THE REPLACEMENT</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Replaces the dense "Up Projection" (W_u).</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Size: [Vocab Size x FFN Intermediate Size]</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stem_embedding <span class="op">=</span> nn.Embedding(config.vocab_size, config.intermediate_size)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act_fn <span class="op">=</span> nn.SiLU()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, input_ids):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step A: Compute the Gate (Contextual)</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        gate_output <span class="op">=</span> <span class="va">self</span>.act_fn(<span class="va">self</span>.gate_proj(x))</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step B: Retrieve Static Memory (Content)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Instead of computing (W_u * x), we just look up the vector for the token.</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        stem_output <span class="op">=</span> <span class="va">self</span>.stem_embedding(input_ids)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step C: Combine (Gating)</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiply static fact by dynamic context.</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        activated_output <span class="op">=</span> gate_output <span class="op">*</span> stem_output</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step D: Project Down</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.down_proj(activated_output)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="deepseek-engram" class="level2">
<h2 class="anchored" data-anchor-id="deepseek-engram">5. DeepSeek Engram</h2>
<p><strong>Ref:</strong> <em>Li et al.&nbsp;(2026)</em></p>
<p>DeepSeek introduces “Conditional Memory” to address the inefficiency of using expensive compute to simulate knowledge retrieval.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/engram_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: The Engram Architecture. The module augments the backbone by retrieving static N-gram memory and fusing it with dynamic hidden states.</figcaption>
</figure>
</div>
<section id="the-architecture-2" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-2">The Architecture</h3>
<ul>
<li><strong>Engram Module:</strong> A dedicated module that augments the neural backbone, structurally separating storage from logic.</li>
<li><strong>Modernized N-Grams:</strong> It uses Hashed N-Grams to map multi-token sequences (e.g., “The capital of”) to static embedding vectors via fast, constant-time <span class="math inline">\(O(1)\)</span> retrieval.</li>
<li><strong>Context-Aware Gating:</strong> The retrieved memory is fused intelligently using the model’s current hidden state to suppress noise.</li>
</ul>
</section>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<p>DeepSeek found a “U-Shaped Scaling Law,” indicating an optimal ratio exists between “Thinking Parameters” (MoE) and “Remembering Parameters” (Engram). Allocating ~20% of the parameter budget to Engram yields better results than a pure MoE model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/engram_scaling.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Sparsity allocation and Engram scaling. Left: Validation loss exhibits a U-shape, with hybrid allocation surpassing Pure MoE.</figcaption>
</figure>
</div>
<p>Surprisingly, offloading static knowledge improves <strong>reasoning benchmarks</strong> (BBH +5.0). By relieving the attention layers from the burden of “memorizing” facts, the model’s depth is freed up to perform complex logic <em>(Li et al., 2026)</em>.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The current “Memory Wall,” where GPU processing speed exceeds memory bandwidth, has created a bottleneck. By decoupling knowledge storage from active parameters, these architectures allow us to scale “memory” massively without exploding GPU requirements, paving the way for models that can “know” more while “thinking” more efficiently.</p>
<hr>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><strong>Ameisen, et al.&nbsp;(2025).</strong> <em>Circuit Tracing: Revealing Computational Graphs in Language Models.</em> Anthropic.</p>
<p><strong>Google DeepMind. (2025).</strong> <em>Gemma 3n Technical Report.</em> Google DeepMind.</p>
<p><strong>He, et al.&nbsp;(2026).</strong> <em>STEM: Scaling Transformers with Embedding Modules.</em> ArXiv: 2601.10639.</p>
<p><strong>Li, et al.&nbsp;(2026).</strong> <em>Conditional Memory via Scalable Lookup.</em> ArXiv: 2601.07372.</p>
<p><strong>Lindsey, et al.&nbsp;(2025).</strong> <em>On the Biology of a Large Language Model.</em> Anthropic.</p>
<p><strong>Templeton, et al.&nbsp;(2024).</strong> <em>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</em> Anthropic.</p>
<p><strong>Zhang, et al.&nbsp;(2025).</strong> <em>On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models.</em> ArXiv: 2512.07783.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>