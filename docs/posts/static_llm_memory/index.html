<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Zhoutong">
<meta name="dcterms.date" content="2026-02-01">

<title>The Rise of Static Memory in LLMs – Zhoutong | AI Research Log</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Zhoutong | AI Research Log</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhoutongfu/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zhoutongfu"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">Posts</a></li><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">The Rise of Static Memory in LLMs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/static_llm_memory/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">The Rise of Static Memory in LLMs</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">1. Background</a>
  <ul class="collapse">
  <li><a href="#the-need-for-scale-knowledge-primitives-limit-reasoning" id="toc-the-need-for-scale-knowledge-primitives-limit-reasoning" class="nav-link" data-scroll-target="#the-need-for-scale-knowledge-primitives-limit-reasoning">The Need for Scale: Knowledge Primitives Limit Reasoning</a></li>
  <li><a href="#the-representation-reality-computing-vs.-retrieving" id="toc-the-representation-reality-computing-vs.-retrieving" class="nav-link" data-scroll-target="#the-representation-reality-computing-vs.-retrieving">The Representation Reality: Computing vs.&nbsp;Retrieving</a></li>
  </ul></li>
  <li><a href="#overview-ple-stem-engram-longcat" id="toc-overview-ple-stem-engram-longcat" class="nav-link" data-scroll-target="#overview-ple-stem-engram-longcat">2. Overview: PLE, STEM, Engram, LongCat</a></li>
  <li><a href="#gemma-3n-per-layer-embeddings" id="toc-gemma-3n-per-layer-embeddings" class="nav-link" data-scroll-target="#gemma-3n-per-layer-embeddings">3. Gemma 3n (Per Layer Embeddings)</a>
  <ul class="collapse">
  <li><a href="#the-architecture" id="toc-the-architecture" class="nav-link" data-scroll-target="#the-architecture">The Architecture</a></li>
  <li><a href="#core-model-code" id="toc-core-model-code" class="nav-link" data-scroll-target="#core-model-code">Core Model Code</a></li>
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  </ul></li>
  <li><a href="#stem-scaling-transformers-with-embedding-modules" id="toc-stem-scaling-transformers-with-embedding-modules" class="nav-link" data-scroll-target="#stem-scaling-transformers-with-embedding-modules">4. STEM (Scaling Transformers with Embedding Modules)</a>
  <ul class="collapse">
  <li><a href="#the-architecture-1" id="toc-the-architecture-1" class="nav-link" data-scroll-target="#the-architecture-1">The Architecture</a></li>
  <li><a href="#core-model-code-1" id="toc-core-model-code-1" class="nav-link" data-scroll-target="#core-model-code-1">Core Model Code</a></li>
  <li><a href="#knowledge-specificity-interpretability" id="toc-knowledge-specificity-interpretability" class="nav-link" data-scroll-target="#knowledge-specificity-interpretability">Knowledge Specificity &amp; Interpretability</a></li>
  <li><a href="#key-findings-1" id="toc-key-findings-1" class="nav-link" data-scroll-target="#key-findings-1">Key Findings</a></li>
  </ul></li>
  <li><a href="#engram-deepseek" id="toc-engram-deepseek" class="nav-link" data-scroll-target="#engram-deepseek">5. Engram (DeepSeek)</a>
  <ul class="collapse">
  <li><a href="#the-architecture-2" id="toc-the-architecture-2" class="nav-link" data-scroll-target="#the-architecture-2">The Architecture</a></li>
  <li><a href="#key-findings-2" id="toc-key-findings-2" class="nav-link" data-scroll-target="#key-findings-2">Key Findings</a></li>
  </ul></li>
  <li><a href="#longcat-scaling-embeddings-outperforms-scaling-experts" id="toc-longcat-scaling-embeddings-outperforms-scaling-experts" class="nav-link" data-scroll-target="#longcat-scaling-embeddings-outperforms-scaling-experts">6. LongCat (Scaling Embeddings Outperforms Scaling Experts)</a>
  <ul class="collapse">
  <li><a href="#the-architecture-3" id="toc-the-architecture-3" class="nav-link" data-scroll-target="#the-architecture-3">The Architecture</a></li>
  <li><a href="#key-findings-3" id="toc-key-findings-3" class="nav-link" data-scroll-target="#key-findings-3">Key Findings</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">Posts</a></li><li class="breadcrumb-item"><a href="../../posts/static_llm_memory/index.html">The Rise of Static Memory in LLMs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">The Rise of Static Memory in LLMs</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">Architecture</div>
    <div class="quarto-category">Memory</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Zhoutong </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 1, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><strong>Topics:</strong> LLM Knowledge, Embedding Scaling, Sparse Architectures</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">1. Background</h2>
<section id="the-need-for-scale-knowledge-primitives-limit-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="the-need-for-scale-knowledge-primitives-limit-reasoning">The Need for Scale: Knowledge Primitives Limit Reasoning</h3>
<p>Improving reasoning remains a primary goal for modern AI, yet recent findings (Zhang et al., 2025 [8]) highlight a critical bottleneck: a model’s reasoning potential is capped by the knowledge “primitives” absorbed during Pre-Training. While Post-Training can refine how a model thinks, it cannot fabricate knowledge from scratch; it merely optimizes the foundation laid earlier. Consequently, achieving substantial breakthroughs in reasoning requires massively scaling the pre-training stage to embed a far richer set of facts and primitives.</p>
</section>
<section id="the-representation-reality-computing-vs.-retrieving" class="level3">
<h3 class="anchored" data-anchor-id="the-representation-reality-computing-vs.-retrieving">The Representation Reality: Computing vs.&nbsp;Retrieving</h3>
<p>Standard Transformers face a major inefficiency when scaling knowledge. Interpretability research from Anthropic [7, 1] reveals that dense models do not store information in a static database; instead, they dynamically “compute” it. Retrieving a simple concept like “The Golden Gate Bridge” triggers a complex, expensive sequence of feature interactions across layers. This architecture conflates storage with processing, forcing the model to burn valuable computational resources (FLOPs) just to “remember” rather than to “think”.</p>
</section>
</section>
<section id="overview-ple-stem-engram-longcat" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overview-ple-stem-engram-longcat">2. Overview: PLE, STEM, Engram, LongCat</h2>
<p>Modern LLMs traditionally conflate “reasoning” (dynamic computation) with “knowledge” (static fact retrieval). We use the same expensive matrix multiplications to calculate math problems as we do to recall that “Paris is the capital of France”.</p>
<p>Four emerging architectures—STEM (Meta/CMU) [6], PLE (Google DeepMind) [3], Engram (DeepSeek) [2], and LongCat (Meituan) [5]—address this by moving static information into Embedding Modules. Instead of computing facts, these models retrieve them. This shift decouples model capacity from inference cost, allowing for massive “memory” scaling without exploding GPU requirements.</p>
<div class="column-page wide-comparison-table">
<table class="caption-top table">
<caption>Comparison of Static Memory Architectures</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Gemma 3n PLE (DeepMind)</th>
<th style="text-align: left;">STEM (Meta/CMU)</th>
<th style="text-align: left;">Engram (DeepSeek)</th>
<th style="text-align: left;">LongCat (Meituan)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Date</strong></td>
<td style="text-align: left;">June 26, 2025</td>
<td style="text-align: left;">Jan 15, 2026</td>
<td style="text-align: left;">Jan 12, 2026</td>
<td style="text-align: left;">Jan 29, 2026</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Core Concept</strong></td>
<td style="text-align: left;">Augmentation: Adds extra embeddings to modulate/augment layers.</td>
<td style="text-align: left;">Replacement: Swaps FFN Up-Projection for a Lookup Table.</td>
<td style="text-align: left;">Complement: Adds a “Memory Module” alongside the Neural Backbone.</td>
<td style="text-align: left;">Input Augmentation: Expands the model’s vocabulary/input capacity rather than depth.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Mechanism</strong></td>
<td style="text-align: left;">Per-Layer Embeddings (Streamed)</td>
<td style="text-align: left;">Token-indexed Lookup (Static)</td>
<td style="text-align: left;">Hashed N-Gram Lookup (Contextual)</td>
<td style="text-align: left;">Input-Level N-Gram Lookup: Averages N-gram vectors into the input.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hardware Goal</strong></td>
<td style="text-align: left;">On-Device Efficiency (Low VRAM)</td>
<td style="text-align: left;">Training Stability &amp; Interpretability</td>
<td style="text-align: left;">Massive Scaling (Memory vs.&nbsp;Logic)</td>
<td style="text-align: left;">Cache Efficiency: Uses “N-gram Caching” to bypass communication bottlenecks.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Key Win</strong></td>
<td style="text-align: left;">Running 8B models on 3GB RAM phones</td>
<td style="text-align: left;">“Surgical” Knowledge Editing</td>
<td style="text-align: left;">+5.0 on Reasoning (BBH) by offloading facts</td>
<td style="text-align: left;">Pareto Efficiency: Beats parameter-equivalent MoEs on Coding &amp; Agentic tasks.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Model Size</strong></td>
<td style="text-align: left;">2B (Effective 4B)</td>
<td style="text-align: left;">1B (Evaluated at 350M &amp; 1B)</td>
<td style="text-align: left;">40B (Tested up to 100B Engram module)</td>
<td style="text-align: left;">68.5B (~31B of which are static N-gram embeddings)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="gemma-3n-per-layer-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="gemma-3n-per-layer-embeddings">3. Gemma 3n (Per Layer Embeddings)</h2>
<section id="the-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture">The Architecture</h3>
<p>Gemma 3n is engineered specifically for <strong>mobile/edge deployment</strong> [3]. Its goal is to fit a model with “8B parameter intelligence” onto a device that only has RAM for a 2B model.</p>
<ul>
<li><strong>PLE (Per Layer Embeddings):</strong> Instead of storing all weights in the GPU/NPU memory (VRAM), it keeps massive embedding tables in the slower <strong>System RAM (CPU)</strong>.</li>
<li><strong>Streaming:</strong> As the neural network processes layer <span class="math inline">\(i\)</span>, the specific embedding for that layer is streamed from the CPU to the NPU just in time.</li>
<li><strong>Mechanism:</strong> PLEs do not usually <em>replace</em> the FFN. Instead, they <strong>modulate</strong> the residual stream or the FFN output. The STEM paper notes that PLEs are often much lower dimension (e.g., 256 dim) compared to the model’s width.</li>
</ul>
</section>
<section id="core-model-code" class="level3">
<h3 class="anchored" data-anchor-id="core-model-code">Core Model Code</h3>
<p>The Per-Layer Embedding (PLE) mechanism works by retrieving a massive, token-specific embedding vector that is pre-sliced to provide a unique “knowledge” input for every decoder layer [9]. As the model processes a token, each layer accesses its specific slice and dynamically gates it using the current hidden state—essentially using the active context to determine how much of the static retrieved knowledge to admit.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The massive "Memory Bank" storing all layer-specific vectors for every token.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Total Size: Vocab_PLE x (Num_Layers * PLE_Dim)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example Dimensions: 2M tokens x (40 layers * 256 dim)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.embed_tokens_per_layer <span class="op">=</span> Gemma3nTextScaledWordEmbedding(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    config.vocab_size_per_layer_input,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    config.num_hidden_layers <span class="op">*</span> config.hidden_size_per_layer_input,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.padding_idx,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    embed_scale<span class="op">=</span>config.hidden_size_per_layer_input<span class="op">**</span><span class="fl">0.5</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_per_layer_inputs(<span class="va">self</span>, input_ids: torch.LongTensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Look up the massive vector and "slice" it for each layer.</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input:  [Batch, Seq_Len]</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output: [Batch, Seq_Len, Num_Layers, PLE_Dim]</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.embed_tokens_per_layer(input_ids).reshape(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>input_ids.shape,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config.num_hidden_layers,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size_per_layer_input,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Inside the decoder loop, we pass the specific slice for this layer:</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># per_layer_input = per_layer_inputs[:, :, layer_idx, :] </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape: [Batch, Seq_Len, PLE_Dim]</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># ... inside the layer's forward pass ...</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># A. GATE: Use the current context (hidden state) to determine "how much" memory to read.</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Project from Model_Dim (e.g., 2560) -&gt; PLE_Dim (e.g., 256)</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>first_prediction <span class="op">=</span> <span class="va">self</span>.per_layer_input_gate(first_prediction)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>first_prediction <span class="op">=</span> <span class="va">self</span>.act_fn(first_prediction)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># B. MODULATE: Inject the retrieved static memory via element-wise multiplication.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Dynamic Context (Gate) * Static Memory (PLE Vector)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>first_prediction <span class="op">=</span> torch.multiply(first_prediction, per_layer_input)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># C. PROJECT: Mix the result back into the main residual stream.</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Project from PLE_Dim (e.g., 256) -&gt; Model_Dim (e.g., 2560)</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>first_prediction <span class="op">=</span> <span class="va">self</span>.per_layer_projection(first_prediction)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>first_prediction <span class="op">=</span> <span class="va">self</span>.post_per_layer_input_norm(first_prediction)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># D. ADD: Add to the model's prediction stream</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>corrected_predictions[<span class="dv">1</span>:] <span class="op">+=</span> first_prediction</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<ol type="1">
<li><strong>VRAM Breakthrough:</strong> Allows running high-capacity models on phones with 3-4GB of RAM.</li>
<li><strong>Parameters vs.&nbsp;Compute:</strong> It massively increases the <em>parameter count</em> (knowledge capacity) without increasing the <em>FLOPs</em> (compute cost) or the <em>active VRAM usage</em>.</li>
</ol>
</section>
</section>
<section id="stem-scaling-transformers-with-embedding-modules" class="level2">
<h2 class="anchored" data-anchor-id="stem-scaling-transformers-with-embedding-modules">4. STEM (Scaling Transformers with Embedding Modules)</h2>
<section id="the-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-1">The Architecture</h3>
<p>STEM [6] identifies that the Up-Projection matrix in a Feed-Forward Network (FFN) acts largely as a “Key” lookup in a Key-Value memory system.</p>
<ul>
<li><strong>Change:</strong> It replaces the dense Up-Projection matrix (<span class="math inline">\(W^u\)</span>) with a static, token-indexed embedding table (<span class="math inline">\(U_{token_id}\)</span>).</li>
<li><strong>Retention:</strong> It keeps the Gate Projection (<span class="math inline">\(W^g\)</span>) and Down Projection (<span class="math inline">\(W^d\)</span>) as dense layers.</li>
<li><strong>Formula:</strong> <span class="math inline">\(y = W_{down}(\text{SiLU}(W_{gate}x) \odot \text{Lookup}(TokenID))\)</span></li>
</ul>
<p><img src="images/stem_schematic.png" class="img-fluid" alt="Schematics of (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM with a single prefetched token embedding."> <em>Figure: Schematics of (a) SwiGLU FFN, (b) MoE FFN, and (c) STEM.</em></p>
</section>
<section id="core-model-code-1" class="level3">
<h3 class="anchored" data-anchor-id="core-model-code-1">Core Model Code</h3>
<p>The snippet below reflects the core logic [10].</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> STEMFFN(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Gate Projection (W_g) - Kept Dense</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determines "how much" of the knowledge to let through based on context.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gate_proj <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Down Projection (W_d) - Kept Dense</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projects the result back to the model's hidden dimension.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_proj <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. STEM Embedding Table (U_l) - THE REPLACEMENT</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Replaces the dense "Up Projection" (W_u).</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Size: [Vocab Size x FFN Intermediate Size]</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is the "Static Memory" containing layer-local facts for each token.</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stem_embedding <span class="op">=</span> nn.Embedding(config.vocab_size, config.intermediate_size)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act_fn <span class="op">=</span> nn.SiLU()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, input_ids):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x shape: [Batch, Seq_Len, Hidden_Dim]</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input_ids shape: [Batch, Seq_Len]</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step A: Compute the Gate (Contextual)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Formula: SiLU(W_g * x)</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The gate looks at the *context* (x) to decide activation.</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        gate_output <span class="op">=</span> <span class="va">self</span>.act_fn(<span class="va">self</span>.gate_proj(x))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step B: Retrieve Static Memory (Content)</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Formula: U_l[t]</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Instead of computing (W_u * x), we just look up the vector for the token.</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This vector represents the "fact" or "value" tied to this specific token in this layer.</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        stem_output <span class="op">=</span> <span class="va">self</span>.stem_embedding(input_ids)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step C: Combine (Gating)</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Formula: Gate * STEM_Embedding</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We multiply the static fact (stem_output) by the dynamic context (gate_output).</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the gate is 0, the model ignores this fact for the current context.</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        activated_output <span class="op">=</span> gate_output <span class="op">*</span> stem_output</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step D: Project Down</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Formula: W_d * (Activated_Output)</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.down_proj(activated_output)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="knowledge-specificity-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-specificity-interpretability">Knowledge Specificity &amp; Interpretability</h3>
<p>The STEM embeddings are inherently linked to individual tokens. Viewing this through a memory lens, this embedding is intended to consolidate the essential information tied to its corresponding token. Consequently, these embeddings can potentially serve as steering vectors.</p>
<p><img src="images/stem_knowledge_edit.png" class="img-fluid" alt="Knowledge injection/edit demonstration. Swapping the PLE for “Spain” to “Germany” flips the generated capital from Madrid to Berlin."> <em>Figure: Knowledge injection/edit demonstration.</em></p>
</section>
<section id="key-findings-1" class="level3">
<h3 class="anchored" data-anchor-id="key-findings-1">Key Findings</h3>
<ol type="1">
<li><strong>Training Stability:</strong> Unlike Mixture-of-Experts (MoE), which suffers from load-balancing issues and loss spikes, STEM trains as stably as dense models.</li>
<li><strong>Efficiency:</strong> Removes ~1/3 of FFN parameters from the active compute path.</li>
<li><strong>Interpretability (The “Killer Feature”):</strong> Because embeddings are tied to specific tokens (e.g., “Spain”), researchers can perform <strong>Knowledge Editing</strong>. By swapping the “Spain” embedding for “Germany” in specific layers, the model can be tricked into “hallucinating” consistent facts (e.g., saying the capital of Spain is Berlin) without changing the input text.</li>
</ol>
</section>
</section>
<section id="engram-deepseek" class="level2">
<h2 class="anchored" data-anchor-id="engram-deepseek">5. Engram (DeepSeek)</h2>
<section id="the-architecture-2" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-2">The Architecture</h3>
<p><strong>DeepSeek introduces “Conditional Memory”</strong> [2] to address a fundamental inefficiency: while Transformers excel at <em>reasoning</em> (via Conditional Computation/MoE), they lack a native primitive for <em>remembering</em>, often wasting expensive compute to simulate knowledge retrieval.</p>
<ul>
<li><strong>Engram Module:</strong> A dedicated module that augments the neural backbone, structurally separating the storage of static patterns from dynamic logic processing.</li>
<li><strong>Modernized N-Grams:</strong> Instead of standard single-token lookups, the model uses <strong>Hashed N-Grams</strong> to map multi-token sequences (e.g., “The capital of”) to static embedding vectors via fast, constant-time <span class="math inline">\(O(1)\)</span> retrieval.</li>
<li><strong>Context-Aware Gating:</strong> The retrieved memory is fused intelligently, not blindly. A gating mechanism uses the model’s current hidden state to evaluate the static fact, integrating it only when relevant to the context while suppressing noise.</li>
</ul>
<p><img src="images/engram_architecture.png" class="img-fluid" alt="The Engram Architecture. The module augments the backbone by retrieving static N-gram memory."> <em>Figure: The Engram Architecture.</em></p>
</section>
<section id="key-findings-2" class="level3">
<h3 class="anchored" data-anchor-id="key-findings-2">Key Findings</h3>
<ol type="1">
<li><strong>The U-Shaped Scaling Law:</strong> DeepSeek found an optimal ratio between “Thinking Parameters” (MoE) and “Remembering Parameters” (Engram). Allocating ~20% of the parameter budget to Engram yields better results than a pure MoE model.</li>
<li><strong>Reasoning Gains:</strong> Surprisingly, offloading static knowledge to Engram improves reasoning benchmarks significantly. <strong>Engram-27B</strong> outperforms a parameter-equivalent <strong>MoE-27B</strong> baseline (e.g., BBH +5.0, MMLU +3.0, MATH +2.4). <em>Why?</em> By relieving the attention layers from the burden of “memorizing” facts, the model’s depth is freed up to perform complex logic.</li>
<li><strong>Needle-in-a-Haystack:</strong> Massive gains in long-context retrieval (84.2% -&gt; 97.0%). The “local dependencies” are handled by the lookup table, leaving the attention mechanism free to scan the full global context.</li>
<li><strong>Zero-Cost “Infinite” Memory:</strong> Engram decouples model size from inference latency. Because the lookups are deterministic (based on N-grams), the system can <strong>prefetch</strong> embeddings from CPU memory (RAM) before the GPU needs them. Result: Offloading a massive 100B-parameter Engram table to host memory incurs a negligible throughput penalty (&lt; 2.8%).</li>
</ol>
<p><img src="images/engram_scaling.png" class="img-fluid" alt="Sparsity allocation and Engram scaling showing the U-shaped scaling law."> <em>Figure: Sparsity allocation and Engram scaling.</em></p>
<p>Unlike previous works that simply demonstrate that external memory improves performance, this paper provides a rigorous ablation study. Through their “Sparsity Allocation” experiments, the authors reveal a fundamental U-shaped scaling law: purely scaling experts (MoE) or purely scaling memory (Engram) is suboptimal. Instead, they identify a precise “sweet spot”—allocating ~20-25% of sparse parameters to static memory minimizes validation loss.</p>
</section>
</section>
<section id="longcat-scaling-embeddings-outperforms-scaling-experts" class="level2">
<h2 class="anchored" data-anchor-id="longcat-scaling-embeddings-outperforms-scaling-experts">6. LongCat (Scaling Embeddings Outperforms Scaling Experts)</h2>
<section id="the-architecture-3" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-3">The Architecture</h3>
<p>LongCat [5] challenges the dominant “Scaling Experts” (MoE) paradigm by proposing <strong>Embedding Scaling</strong>. Instead of adding more “brains” (experts) to the Feed-Forward Networks, it massively scales the <strong>Input Embedding Layer</strong> to capture rich local context.</p>
<ul>
<li><strong>Input-Only Integration:</strong> Unlike STEM (which replaces FFNs) or Engram (which runs parallel to the backbone), LongCat adds N-gram embeddings <em>only</em> at the input level.</li>
<li><strong>Mechanism:</strong> The model retrieves massive N-gram vectors (2-grams, 3-grams) and <strong>averages</strong> them with the standard token embedding to create a “super-charged” input vector <span class="math inline">\(e_i\)</span>.</li>
<li><strong>Standard Backbone:</strong> The rest of the Transformer (Attention &amp; FFNs) remains unchanged. It simply processes these denser, richer input vectors.</li>
</ul>
<p><img src="images/longcat_ngram.png" class="img-fluid" alt="The architecture of a N-gram Embedding layer from LongCat."> <em>Figure: The architecture of a N-gram Embedding layer.</em></p>
</section>
<section id="key-findings-3" class="level3">
<h3 class="anchored" data-anchor-id="key-findings-3">Key Findings</h3>
<ul>
<li><strong>The “Fat Embedding” Pareto Frontier:</strong> The authors found that once the number of experts reaches a certain point, adding <strong>more memory</strong> (embeddings) yields better returns than adding more experts.</li>
<li><strong>The 50% Allocation Rule:</strong> The optimal design allocates roughly <strong>50% of the total parameter budget</strong> to these static embeddings. <em>Example:</em> <strong>LongCat-Flash-Lite</strong> is a 68.5B parameter model, but <strong>~31.4B</strong> parameters are just the embedding table.</li>
<li><strong>Wide vs.&nbsp;Deep:</strong> This method works best in <strong>wider</strong> models. In very deep models, the signal from the input embeddings tends to fade (“wash out”) as it propagates through many layers.</li>
<li><strong>Performance:</strong> The 68.5B model (with only ~3B active params) outperformed parameter-equivalent MoE baselines, showing particular strength in <strong>coding and agentic tasks</strong> where local context is critical.</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>Ameisen, E. et al.&nbsp;(2025). <em>Circuit Tracing: Revealing Computational Graphs in Language Models.</em> Anthropic.</li>
<li>Cheng, X. et al.&nbsp;(2026). <em>Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models.</em> arXiv:2601.07372.</li>
<li>Google DeepMind (2025). <em>Gemma 3 Technical Report.</em> arXiv:2503.19786. Model card: <a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a>.</li>
<li>Lindsey, J. et al.&nbsp;(2025). <em>On the Biology of a Large Language Model.</em> Anthropic.</li>
<li>LongCat (2026). <em>Scaling Embeddings Outperforms Scaling Experts in Language Models.</em> arXiv:2601.21204.</li>
<li>Sadhukhan, R. et al.&nbsp;(2026). <em>STEM: Scaling Transformers with Embedding Modules.</em> arXiv:2601.10639.</li>
<li>Templeton, A. et al.&nbsp;(2024). <em>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</em> Anthropic.</li>
<li>Zhang, J. et al.&nbsp;(2025). <em>On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models.</em> arXiv:2512.07783.</li>
<li>Google DeepMind. <em>Gemma 3n model code.</em> <code>gemma3n/modeling_gemma3n.py</code>.</li>
<li>Lingua / STEM. <em>STEM model code.</em> <code>lingua/stem.py</code>.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>